{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "085e902d-4e6a-48cd-b1f5-580a443f7dcb",
   "metadata": {},
   "source": [
    "# New Idea!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dc659e77-a761-4d38-900c-df6514abb150",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms, datasets\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2d2b9ed5-b41b-4809-bd05-63941952f00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "232192ee-6814-4808-872b-688e2a5071e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/brendenlake/omniglot/master/python/images_background.zip to ./data\\omniglot-py\\images_background.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 9464212/9464212 [00:23<00:00, 406141.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\omniglot-py\\images_background.zip to ./data\\omniglot-py\n",
      "Downloading https://raw.githubusercontent.com/brendenlake/omniglot/master/python/images_evaluation.zip to ./data\\omniglot-py\\images_evaluation.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 6462886/6462886 [00:28<00:00, 229593.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\omniglot-py\\images_evaluation.zip to ./data\\omniglot-py\n"
     ]
    }
   ],
   "source": [
    "# Image Transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.Resize((28, 28)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Load Omniglot Dataset\n",
    "train_dataset = datasets.Omniglot(\n",
    "    root='./data',\n",
    "    background=True,\n",
    "    transform=transform,\n",
    "    download=True\n",
    ")\n",
    "\n",
    "test_dataset = datasets.Omniglot(\n",
    "    root='./data',\n",
    "    background=False,\n",
    "    transform=transform,\n",
    "    download=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f05d4aa4-5a20-4b78-b6eb-260bd23d0542",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_episode(dataset, n_way, k_shot, k_query):\n",
    "    # Get the list of labels\n",
    "    if hasattr(dataset, 'targets'):\n",
    "        labels = dataset.targets\n",
    "    else:\n",
    "        labels = [label for _, label in dataset._flat_character_images]\n",
    "    \n",
    "    # Get unique classes\n",
    "    classes = list(set(labels))\n",
    "    \n",
    "    # Build a mapping from class label to indices of images\n",
    "    class_to_indices = {cls: [] for cls in classes}\n",
    "    for idx, label in enumerate(labels):\n",
    "        class_to_indices[label].append(idx)\n",
    "    \n",
    "    # Filter out classes that don't have enough samples\n",
    "    classes_with_enough_samples = [cls for cls in classes if len(class_to_indices[cls]) >= k_shot + k_query]\n",
    "    \n",
    "    if len(classes_with_enough_samples) < n_way:\n",
    "        raise ValueError(f\"Not enough classes with at least {k_shot + k_query} samples. \"\n",
    "                         f\"Available: {len(classes_with_enough_samples)}, Required: {n_way}\")\n",
    "    \n",
    "    selected_classes = random.sample(classes_with_enough_samples, n_way)\n",
    "    support_images = []\n",
    "    query_images = []\n",
    "    support_labels = []\n",
    "    query_labels = []\n",
    "    class_to_idx = {cls: idx for idx, cls in enumerate(selected_classes)}\n",
    "    \n",
    "    for cls in selected_classes:\n",
    "        indices = class_to_indices[cls]\n",
    "        selected_indices = random.sample(indices, k_shot + k_query)\n",
    "        support_indices = selected_indices[:k_shot]\n",
    "        query_indices = selected_indices[k_shot:]\n",
    "        \n",
    "        for idx in support_indices:\n",
    "            image, _ = dataset[idx]\n",
    "            support_images.append(image)\n",
    "            support_labels.append(class_to_idx[cls])\n",
    "        \n",
    "        for idx in query_indices:\n",
    "            image, _ = dataset[idx]\n",
    "            query_images.append(image)\n",
    "            query_labels.append(class_to_idx[cls])\n",
    "    \n",
    "    support_images = torch.stack(support_images)\n",
    "    query_images = torch.stack(query_images)\n",
    "    support_labels = torch.tensor(support_labels)\n",
    "    query_labels = torch.tensor(query_labels)\n",
    "    \n",
    "    return support_images, support_labels, query_images, query_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "460d5e50-cdde-4838-9750-72da1c67f0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplified Capsule Network\n",
    "class CapsuleNet(nn.Module):\n",
    "    def __init__(self, output_dim):\n",
    "        super(CapsuleNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 256, kernel_size=9, stride=1)\n",
    "        self.primary_caps = nn.Conv2d(256, 8 * 32, kernel_size=9, stride=2)\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.primary_caps(x)\n",
    "        batch_size = x.size(0)\n",
    "        x = x.view(batch_size, 32 * 8 * 8, 8)\n",
    "        x = self.squash(x)\n",
    "        x = x.view(batch_size, -1)\n",
    "        return x[:, :self.output_dim]\n",
    "\n",
    "    def squash(self, x):\n",
    "        s_squared_norm = (x ** 2).sum(dim=-1, keepdim=True)\n",
    "        scale = s_squared_norm / (1 + s_squared_norm)\n",
    "        return scale * x / torch.sqrt(s_squared_norm + 1e-8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "00fd979c-a060-4f9b-ae5b-02e4c050654b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prototypical Network\n",
    "class PrototypicalNetwork(nn.Module):\n",
    "    def __init__(self, feature_extractor):\n",
    "        super(PrototypicalNetwork, self).__init__()\n",
    "        self.feature_extractor = feature_extractor\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeddings = self.feature_extractor(x)\n",
    "        return embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c04abffa-d2a0-4ed5-be28-5640f38fe0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_prototypes(embeddings, labels, n_way, k_shot):\n",
    "    prototypes = []\n",
    "    for i in range(n_way):\n",
    "        class_embeddings = embeddings[labels == i]\n",
    "        class_prototype = class_embeddings.mean(0)\n",
    "        prototypes.append(class_prototype)\n",
    "    prototypes = torch.stack(prototypes)\n",
    "    return prototypes\n",
    "\n",
    "def prototypical_loss(query_embeddings, query_labels, prototypes):\n",
    "    distances = euclidean_distances(query_embeddings, prototypes)\n",
    "    log_p_y = F.log_softmax(-distances, dim=1)\n",
    "    loss = -log_p_y.gather(1, query_labels.unsqueeze(1)).mean()\n",
    "    return loss\n",
    "\n",
    "def euclidean_distances(a, b):\n",
    "    n = a.size(0)\n",
    "    m = b.size(0)\n",
    "    a = a.unsqueeze(1).expand(n, m, -1)\n",
    "    b = b.unsqueeze(0).expand(n, m, -1)\n",
    "    distances = ((a - b) ** 2).sum(2)\n",
    "    return distances\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d2a18360-21cb-4e40-831e-7348ebafc9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_set_recognition(embeddings, prototypes, threshold):\n",
    "    distances = euclidean_distances(embeddings, prototypes)\n",
    "    min_distances, predicted_labels = distances.min(1)\n",
    "    is_known = min_distances <= threshold\n",
    "    predicted_labels[~is_known] = -1  # Assign -1 to unknown classes\n",
    "    return predicted_labels, min_distances\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e6ea490f-660d-436a-a72c-42c45cbc6a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_prototypical_network(model, dataset, optimizer, num_episodes=1000, n_way=5, k_shot=5, k_query=15):\n",
    "    model.train()\n",
    "    for episode in range(num_episodes):\n",
    "        support_images, support_labels, query_images, query_labels = create_episode(\n",
    "            dataset, n_way, k_shot, k_query)\n",
    "        \n",
    "        support_images = support_images.to(device)\n",
    "        query_images = query_images.to(device)\n",
    "        support_labels = support_labels.to(device)\n",
    "        query_labels = query_labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Combine support and query images\n",
    "        images = torch.cat([support_images, query_images], dim=0)\n",
    "        embeddings = model(images)\n",
    "\n",
    "        # Split embeddings\n",
    "        support_embeddings = embeddings[:n_way * k_shot]\n",
    "        query_embeddings = embeddings[n_way * k_shot:]\n",
    "\n",
    "        # Compute prototypes\n",
    "        prototypes = compute_prototypes(support_embeddings, support_labels, n_way, k_shot)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = prototypical_loss(query_embeddings, query_labels, prototypes)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (episode + 1) % 100 == 0:\n",
    "            print(f\"Episode {episode+1}/{num_episodes}, Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6547e5f2-9d42-4030-b9e5-0d85c8eb5280",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_prototypical_network(model, train_dataset, test_dataset, n_way=5, k_shot=5, k_query=15, threshold=1.0):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Support set from training dataset\n",
    "        support_images, support_labels, _, _ = create_episode(\n",
    "            train_dataset, n_way, k_shot, k_query=0)\n",
    "        support_images = support_images.to(device)\n",
    "        support_labels = support_labels.to(device)\n",
    "        support_embeddings = model(support_images)\n",
    "        prototypes = compute_prototypes(support_embeddings, support_labels, n_way, k_shot)\n",
    "\n",
    "        # Query set from test dataset (unseen classes)\n",
    "        _, _, query_images, query_labels = create_episode(\n",
    "            test_dataset, n_way, k_shot=0, k_query=k_query)\n",
    "        query_images = query_images.to(device)\n",
    "        query_labels = query_labels.to(device)\n",
    "\n",
    "        query_embeddings = model(query_images)\n",
    "\n",
    "        # Open-set recognition\n",
    "        predicted_labels, min_distances = open_set_recognition(query_embeddings, prototypes, threshold)\n",
    "\n",
    "        # Map unknown classes to -1\n",
    "        true_labels = torch.full_like(query_labels, -1)\n",
    "        accuracy = (predicted_labels == true_labels).float().mean().item()\n",
    "        print(f\"Test Accuracy (Open-Set Recognition): {accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5827d475-ef73-4157-9e31-2040e33baa8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model and optimizer\n",
    "feature_extractor = CapsuleNet(output_dim=64).to(device)\n",
    "model = PrototypicalNetwork(feature_extractor).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ed6f98bd-64de-4cd5-a0e5-b6a7880aac52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total classes: 964\n",
      "Classes with at least 15 samples: 964\n"
     ]
    }
   ],
   "source": [
    "def count_classes_with_enough_samples(dataset, min_samples):\n",
    "    # Get the list of labels\n",
    "    if hasattr(dataset, 'targets'):\n",
    "        labels = dataset.targets\n",
    "    else:\n",
    "        labels = [label for _, label in dataset._flat_character_images]\n",
    "    \n",
    "    # Build a mapping from class label to indices of images\n",
    "    from collections import Counter\n",
    "    label_counts = Counter(labels)\n",
    "    classes_with_enough_samples = [cls for cls, count in label_counts.items() if count >= min_samples]\n",
    "    print(f\"Total classes: {len(set(labels))}\")\n",
    "    print(f\"Classes with at least {min_samples} samples: {len(classes_with_enough_samples)}\")\n",
    "    return classes_with_enough_samples\n",
    "\n",
    "# Check the number of classes with at least k_shot + k_query samples\n",
    "classes_available = count_classes_with_enough_samples(train_dataset, k_shot + k_query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "74e14601-9a31-472b-8df1-592684c37cd5",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[50, 2048, 8]' is invalid for input of size 460800",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[47], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m k_shot \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m      \u001b[38;5;66;03m# Number of support samples per class\u001b[39;00m\n\u001b[0;32m      5\u001b[0m k_query \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m     \u001b[38;5;66;03m# Number of query samples per class\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[43mtrain_prototypical_network\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_episodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_way\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk_shot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk_query\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[43], line 16\u001b[0m, in \u001b[0;36mtrain_prototypical_network\u001b[1;34m(model, dataset, optimizer, num_episodes, n_way, k_shot, k_query)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Combine support and query images\u001b[39;00m\n\u001b[0;32m     15\u001b[0m images \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([support_images, query_images], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m---> 16\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Split embeddings\u001b[39;00m\n\u001b[0;32m     19\u001b[0m support_embeddings \u001b[38;5;241m=\u001b[39m embeddings[:n_way \u001b[38;5;241m*\u001b[39m k_shot]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[40], line 8\u001b[0m, in \u001b[0;36mPrototypicalNetwork.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m----> 8\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature_extractor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m embeddings\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[39], line 13\u001b[0m, in \u001b[0;36mCapsuleNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     11\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprimary_caps(x)\n\u001b[0;32m     12\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m---> 13\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msquash(x)\n\u001b[0;32m     15\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(batch_size, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: shape '[50, 2048, 8]' is invalid for input of size 460800"
     ]
    }
   ],
   "source": [
    "# Training parameters\n",
    "num_episodes = 1000\n",
    "n_way = 5       # Number of classes per episode\n",
    "k_shot = 5      # Number of support samples per class\n",
    "k_query = 5     # Number of query samples per class\n",
    "\n",
    "train_prototypical_network(model, train_dataset, optimizer, num_episodes, n_way, k_shot, k_query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f729238-0da2-47e2-888d-9e2df56c77b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing parameters\n",
    "threshold = 1.0  # Threshold for open-set recognition\n",
    "\n",
    "test_prototypical_network(model, train_dataset, test_dataset, n_way, k_shot, k_query, threshold)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842f18b0-317d-4534-9c22-ebc57437b831",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GuidedBackpropReLUModel:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.model.eval()\n",
    "        self.update_relus()\n",
    "\n",
    "    def update_relus(self):\n",
    "        def recursive_relu_apply(module_top):\n",
    "            for idx, module in module_top._modules.items():\n",
    "                if isinstance(module, nn.ReLU):\n",
    "                    module_top._modules[idx] = GuidedReLU()\n",
    "                elif len(module._modules) > 0:\n",
    "                    recursive_relu_apply(module)\n",
    "\n",
    "        recursive_relu_apply(self.model)\n",
    "\n",
    "    def generate_gradients(self, input_image, target_class):\n",
    "        input_image = input_image.to(device)\n",
    "        input_image.requires_grad = True\n",
    "        output = self.model(input_image)\n",
    "        self.model.zero_grad()\n",
    "        grad_target_map = torch.zeros(output.shape, dtype=torch.float).to(device)\n",
    "        grad_target_map[0][target_class] = 1\n",
    "        output.backward(grad_target_map)\n",
    "        gradients_as_arr = input_image.grad.data.cpu().numpy()[0]\n",
    "        return gradients_as_arr\n",
    "\n",
    "class GuidedReLU(nn.Module):\n",
    "    def forward(self, input):\n",
    "        positive_mask = (input > 0).type_as(input)\n",
    "        output = torch.relu(input)\n",
    "        return output * positive_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd8318c-8c36-4823-b0e2-1fd87f600fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_with_explainability(model, image, prototypes, threshold, explainer):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        embedding = model(image.to(device))\n",
    "    predicted_labels, distances = open_set_recognition(embedding, prototypes, threshold)\n",
    "    if predicted_labels.item() == -1:\n",
    "        result = \"Unknown\"\n",
    "        target_class = distances.argmin().item()\n",
    "    else:\n",
    "        result = f\"Class {predicted_labels.item()}\"\n",
    "        target_class = predicted_labels.item()\n",
    "    # Generate Saliency Map\n",
    "    saliency_map = explainer.generate_gradients(image, target_class)\n",
    "    saliency_map = np.transpose(saliency_map, (1, 2, 0))\n",
    "    saliency_map = np.mean(np.abs(saliency_map), axis=2)\n",
    "    return result, saliency_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99ed9d0-69dd-4223-aa2b-7fe18e81702b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the explainer\n",
    "explainer = GuidedBackpropReLUModel(model)\n",
    "\n",
    "# Load a test image from the test dataset\n",
    "test_image, _ = test_dataset[0]\n",
    "test_image = test_image.unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "# Run inference\n",
    "result, saliency_map = inference_with_explainability(model, test_image, prototypes, threshold=1.0, explainer=explainer)\n",
    "print(\"Inference Result:\", result)\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(test_image.squeeze().cpu().numpy(), cmap='gray')\n",
    "plt.imshow(saliency_map, cmap='hot', alpha=0.5)\n",
    "plt.title(\"Saliency Map\")\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3cebc19e-60b7-4ba2-8997-df238b1b4270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Episode 100/1000, Loss: 0.5493\n",
      "Episode 200/1000, Loss: 0.5843\n",
      "Episode 300/1000, Loss: 0.8364\n",
      "Episode 400/1000, Loss: 0.6925\n",
      "Episode 500/1000, Loss: 0.7529\n",
      "Episode 600/1000, Loss: 0.0610\n",
      "Episode 700/1000, Loss: 0.1656\n",
      "Episode 800/1000, Loss: 0.0920\n",
      "Episode 900/1000, Loss: 0.4748\n",
      "Episode 1000/1000, Loss: 0.3606\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "stack expects a non-empty TensorList",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[48], line 236\u001b[0m\n\u001b[0;32m    234\u001b[0m \u001b[38;5;66;03m# 14. Test the Model\u001b[39;00m\n\u001b[0;32m    235\u001b[0m threshold \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m  \u001b[38;5;66;03m# Adjust based on validation\u001b[39;00m\n\u001b[1;32m--> 236\u001b[0m \u001b[43mtest_prototypical_network\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_class_to_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    237\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mtest_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_class_to_indices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_way\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk_shot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    239\u001b[0m \u001b[38;5;66;03m# 15. Explainability Module (Using Guided Backpropagation)\u001b[39;00m\n\u001b[0;32m    240\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mGuidedBackpropReLUModel\u001b[39;00m(nn\u001b[38;5;241m.\u001b[39mModule):\n",
      "Cell \u001b[1;32mIn[48], line 197\u001b[0m, in \u001b[0;36mtest_prototypical_network\u001b[1;34m(model, train_dataset, train_class_to_indices, test_dataset, test_class_to_indices, n_way, k_shot, k_query, threshold)\u001b[0m\n\u001b[0;32m    194\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m    196\u001b[0m     \u001b[38;5;66;03m# Create support set from training dataset\u001b[39;00m\n\u001b[1;32m--> 197\u001b[0m     support_images, support_labels, _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_episode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    198\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_class_to_indices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_way\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk_shot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    199\u001b[0m     support_images \u001b[38;5;241m=\u001b[39m support_images\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m    200\u001b[0m     support_labels \u001b[38;5;241m=\u001b[39m support_labels\u001b[38;5;241m.\u001b[39mto(device)\n",
      "Cell \u001b[1;32mIn[48], line 148\u001b[0m, in \u001b[0;36mcreate_episode\u001b[1;34m(dataset, class_to_indices, n_way, k_shot, k_query)\u001b[0m\n\u001b[0;32m    145\u001b[0m         query_labels\u001b[38;5;241m.\u001b[39mappend(class_to_idx[\u001b[38;5;28mcls\u001b[39m])\n\u001b[0;32m    147\u001b[0m support_images \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(support_images)\n\u001b[1;32m--> 148\u001b[0m query_images \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_images\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    149\u001b[0m support_labels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(support_labels)\n\u001b[0;32m    150\u001b[0m query_labels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(query_labels)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: stack expects a non-empty TensorList"
     ]
    }
   ],
   "source": [
    "# 1. Import Libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms, datasets\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "# 2. Set Device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# 3. Prepare the Omniglot Dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.Resize((28, 28)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "train_dataset = datasets.Omniglot(\n",
    "    root='./data',\n",
    "    background=True,\n",
    "    transform=transform,\n",
    "    download=True\n",
    ")\n",
    "\n",
    "test_dataset = datasets.Omniglot(\n",
    "    root='./data',\n",
    "    background=False,\n",
    "    transform=transform,\n",
    "    download=True\n",
    ")\n",
    "\n",
    "# 4. Extract Labels and Build Class-to-Indices Mapping\n",
    "def extract_labels_and_build_mapping(dataset):\n",
    "    labels = []\n",
    "    for idx in range(len(dataset)):\n",
    "        _, label = dataset[idx]\n",
    "        labels.append(label)\n",
    "    class_to_indices = defaultdict(list)\n",
    "    for idx, label in enumerate(labels):\n",
    "        class_to_indices[label].append(idx)\n",
    "    return labels, class_to_indices\n",
    "\n",
    "# Extract labels and mappings for train and test datasets\n",
    "train_labels, train_class_to_indices = extract_labels_and_build_mapping(train_dataset)\n",
    "test_labels, test_class_to_indices = extract_labels_and_build_mapping(test_dataset)\n",
    "\n",
    "# 5. Define the Capsule Network\n",
    "class CapsuleNet(nn.Module):\n",
    "    def __init__(self, output_dim):\n",
    "        super(CapsuleNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 256, kernel_size=9, stride=1)\n",
    "        self.primary_caps = nn.Conv2d(256, 8 * 32, kernel_size=9, stride=2)\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.primary_caps(x)\n",
    "        batch_size = x.size(0)\n",
    "        x = x.view(batch_size, -1, 8)\n",
    "        x = self.squash(x)\n",
    "        x = x.view(batch_size, -1)\n",
    "        return x[:, :self.output_dim]\n",
    "\n",
    "    def squash(self, x):\n",
    "        s_squared_norm = (x ** 2).sum(dim=-1, keepdim=True)\n",
    "        scale = s_squared_norm / (1 + s_squared_norm)\n",
    "        return scale * x / torch.sqrt(s_squared_norm + 1e-8)\n",
    "\n",
    "# 6. Define the Prototypical Network\n",
    "class PrototypicalNetwork(nn.Module):\n",
    "    def __init__(self, feature_extractor):\n",
    "        super(PrototypicalNetwork, self).__init__()\n",
    "        self.feature_extractor = feature_extractor\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeddings = self.feature_extractor(x)\n",
    "        return embeddings\n",
    "\n",
    "# 7. Helper Functions\n",
    "def compute_prototypes(embeddings, labels, n_way, k_shot):\n",
    "    prototypes = []\n",
    "    for i in range(n_way):\n",
    "        class_embeddings = embeddings[labels == i]\n",
    "        class_prototype = class_embeddings.mean(0)\n",
    "        prototypes.append(class_prototype)\n",
    "    prototypes = torch.stack(prototypes)\n",
    "    return prototypes\n",
    "\n",
    "def prototypical_loss(query_embeddings, query_labels, prototypes):\n",
    "    distances = euclidean_distances(query_embeddings, prototypes)\n",
    "    log_p_y = F.log_softmax(-distances, dim=1)\n",
    "    loss = -log_p_y.gather(1, query_labels.unsqueeze(1)).mean()\n",
    "    return loss\n",
    "\n",
    "def euclidean_distances(a, b):\n",
    "    n = a.size(0)\n",
    "    m = b.size(0)\n",
    "    a = a.unsqueeze(1).expand(n, m, -1)\n",
    "    b = b.unsqueeze(0).expand(n, m, -1)\n",
    "    distances = ((a - b) ** 2).sum(2)\n",
    "    return distances\n",
    "\n",
    "def open_set_recognition(embeddings, prototypes, threshold):\n",
    "    distances = euclidean_distances(embeddings, prototypes)\n",
    "    min_distances, predicted_labels = distances.min(1)\n",
    "    is_known = min_distances <= threshold\n",
    "    predicted_labels[~is_known] = -1  # Assign -1 to unknown classes\n",
    "    return predicted_labels, min_distances\n",
    "\n",
    "# 8. Create Episode Function\n",
    "def create_episode(dataset, class_to_indices, n_way, k_shot, k_query):\n",
    "    # Filter out classes that don't have enough samples\n",
    "    classes_with_enough_samples = [cls for cls, idxs in class_to_indices.items() if len(idxs) >= k_shot + k_query]\n",
    "    \n",
    "    if len(classes_with_enough_samples) < n_way:\n",
    "        raise ValueError(f\"Not enough classes with at least {k_shot + k_query} samples. \"\n",
    "                         f\"Available: {len(classes_with_enough_samples)}, Required: {n_way}\")\n",
    "    \n",
    "    selected_classes = random.sample(classes_with_enough_samples, n_way)\n",
    "    support_images = []\n",
    "    query_images = []\n",
    "    support_labels = []\n",
    "    query_labels = []\n",
    "    class_to_idx = {cls: idx for idx, cls in enumerate(selected_classes)}\n",
    "    \n",
    "    for cls in selected_classes:\n",
    "        indices = class_to_indices[cls]\n",
    "        selected_indices = random.sample(indices, k_shot + k_query)\n",
    "        support_indices = selected_indices[:k_shot]\n",
    "        query_indices = selected_indices[k_shot:]\n",
    "        \n",
    "        for idx in support_indices:\n",
    "            image, _ = dataset[idx]\n",
    "            support_images.append(image)\n",
    "            support_labels.append(class_to_idx[cls])\n",
    "        \n",
    "        for idx in query_indices:\n",
    "            image, _ = dataset[idx]\n",
    "            query_images.append(image)\n",
    "            query_labels.append(class_to_idx[cls])\n",
    "    \n",
    "    support_images = torch.stack(support_images)\n",
    "    query_images = torch.stack(query_images)\n",
    "    support_labels = torch.tensor(support_labels)\n",
    "    query_labels = torch.tensor(query_labels)\n",
    "    \n",
    "    return support_images, support_labels, query_images, query_labels\n",
    "\n",
    "# 9. Training Function\n",
    "def train_prototypical_network(model, dataset, optimizer, class_to_indices, num_episodes=1000, n_way=5, k_shot=5, k_query=5):\n",
    "    model.train()\n",
    "    for episode in range(num_episodes):\n",
    "        try:\n",
    "            support_images, support_labels, query_images, query_labels = create_episode(\n",
    "                dataset, class_to_indices, n_way, k_shot, k_query)\n",
    "        except ValueError as e:\n",
    "            print(e)\n",
    "            continue  # Skip this episode if not enough classes\n",
    "        \n",
    "        support_images = support_images.to(device)\n",
    "        query_images = query_images.to(device)\n",
    "        support_labels = support_labels.to(device)\n",
    "        query_labels = query_labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Combine support and query images\n",
    "        images = torch.cat([support_images, query_images], dim=0)\n",
    "        embeddings = model(images)\n",
    "\n",
    "        # Split embeddings\n",
    "        support_embeddings = embeddings[:n_way * k_shot]\n",
    "        query_embeddings = embeddings[n_way * k_shot:]\n",
    "\n",
    "        # Compute prototypes\n",
    "        prototypes = compute_prototypes(support_embeddings, support_labels, n_way, k_shot)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = prototypical_loss(query_embeddings, query_labels, prototypes)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (episode + 1) % 100 == 0:\n",
    "            print(f\"Episode {episode+1}/{num_episodes}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "# 10. Testing Function\n",
    "def test_prototypical_network(model, train_dataset, train_class_to_indices, test_dataset, test_class_to_indices,\n",
    "                              n_way=5, k_shot=5, k_query=5, threshold=1.0):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Create support set from training dataset\n",
    "        support_images, support_labels, _, _ = create_episode(\n",
    "            train_dataset, train_class_to_indices, n_way, k_shot, k_query=0)\n",
    "        support_images = support_images.to(device)\n",
    "        support_labels = support_labels.to(device)\n",
    "        support_embeddings = model(support_images)\n",
    "        prototypes = compute_prototypes(support_embeddings, support_labels, n_way, k_shot)\n",
    "\n",
    "        # Create query set from test dataset (unseen classes)\n",
    "        _, _, query_images, query_labels = create_episode(\n",
    "            test_dataset, test_class_to_indices, n_way, k_shot=0, k_query=k_query)\n",
    "        query_images = query_images.to(device)\n",
    "        query_labels = query_labels.to(device)\n",
    "\n",
    "        query_embeddings = model(query_images)\n",
    "\n",
    "        # Open-set recognition\n",
    "        predicted_labels, min_distances = open_set_recognition(query_embeddings, prototypes, threshold)\n",
    "\n",
    "        # Map unknown classes to -1\n",
    "        true_labels = torch.full_like(query_labels, -1)\n",
    "        accuracy = (predicted_labels == true_labels).float().mean().item()\n",
    "        print(f\"Test Accuracy (Open-Set Recognition): {accuracy * 100:.2f}%\")\n",
    "\n",
    "# 11. Initialize Model and Optimizer\n",
    "feature_extractor = CapsuleNet(output_dim=64).to(device)\n",
    "model = PrototypicalNetwork(feature_extractor).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 12. Training Parameters\n",
    "num_episodes = 1000\n",
    "n_way = 5\n",
    "k_shot = 5\n",
    "k_query = 5\n",
    "\n",
    "# 13. Train the Model\n",
    "train_prototypical_network(model, train_dataset, optimizer, train_class_to_indices, num_episodes, n_way, k_shot, k_query)\n",
    "\n",
    "# 14. Test the Model\n",
    "threshold = 1.0  # Adjust based on validation\n",
    "test_prototypical_network(model, train_dataset, train_class_to_indices,\n",
    "                          test_dataset, test_class_to_indices, n_way, k_shot, k_query, threshold)\n",
    "\n",
    "# 15. Explainability Module (Using Guided Backpropagation)\n",
    "class GuidedBackpropReLUModel(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super(GuidedBackpropReLUModel, self).__init__()\n",
    "        self.model = model\n",
    "        self.relu_outputs = []\n",
    "        self.model.eval()\n",
    "        self.update_relus()\n",
    "\n",
    "    def update_relus(self):\n",
    "        def recursive_relu_apply(module_top):\n",
    "            for idx, module in module_top._modules.items():\n",
    "                if isinstance(module, nn.ReLU):\n",
    "                    module_top._modules[idx] = GuidedReLU()\n",
    "                elif len(module._modules) > 0:\n",
    "                    recursive_relu_apply(module)\n",
    "\n",
    "        recursive_relu_apply(self.model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def generate_gradients(self, input_image, target_class):\n",
    "        input_image = input_image.to(device)\n",
    "        input_image.requires_grad = True\n",
    "        output = self.forward(input_image)\n",
    "        self.model.zero_grad()\n",
    "        grad_target_map = torch.zeros_like(output)\n",
    "        grad_target_map[0][target_class] = 1\n",
    "        output.backward(grad_target_map)\n",
    "        gradients_as_arr = input_image.grad.data.cpu().numpy()[0]\n",
    "        return gradients_as_arr\n",
    "\n",
    "class GuidedReLU(nn.Module):\n",
    "    def forward(self, input):\n",
    "        positive_mask = (input > 0).type_as(input)\n",
    "        output = F.relu(input)\n",
    "        return output * positive_mask\n",
    "\n",
    "# 16. Inference with Explainability\n",
    "def inference_with_explainability(model, image, prototypes, threshold, explainer):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        embedding = model(image.to(device))\n",
    "    predicted_labels, distances = open_set_recognition(embedding, prototypes, threshold)\n",
    "    if predicted_labels.item() == -1:\n",
    "        result = \"Unknown\"\n",
    "        target_class = distances.argmin().item()\n",
    "    else:\n",
    "        result = f\"Class {predicted_labels.item()}\"\n",
    "        target_class = predicted_labels.item()\n",
    "    # Generate Saliency Map\n",
    "    saliency_map = explainer.generate_gradients(image, target_class)\n",
    "    saliency_map = np.transpose(saliency_map, (1, 2, 0))\n",
    "    saliency_map = np.mean(np.abs(saliency_map), axis=2)\n",
    "    return result, saliency_map\n",
    "\n",
    "# 17. Run Inference on a Test Image\n",
    "# Initialize the explainer\n",
    "explainer = GuidedBackpropReLUModel(model.feature_extractor)\n",
    "\n",
    "# Select a test image\n",
    "test_image, _ = test_dataset[0]\n",
    "test_image = test_image.unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "# Run inference\n",
    "result, saliency_map = inference_with_explainability(model, test_image, prototypes, threshold=1.0, explainer=explainer)\n",
    "print(\"Inference Result:\", result)\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(test_image.squeeze().cpu().numpy(), cmap='gray')\n",
    "plt.imshow(saliency_map, cmap='hot', alpha=0.5)\n",
    "plt.title(\"Saliency Map\")\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ab251eda-6e27-45c9-8583-fe4e589affc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Episode 100/1000, Loss: 0.8494\n",
      "Episode 200/1000, Loss: 0.9689\n",
      "Episode 300/1000, Loss: 0.4268\n",
      "Episode 400/1000, Loss: 0.2850\n",
      "Episode 500/1000, Loss: 0.3128\n",
      "Episode 600/1000, Loss: 0.4608\n",
      "Episode 700/1000, Loss: 1.5114\n",
      "Episode 800/1000, Loss: 0.1305\n",
      "Episode 900/1000, Loss: 0.3142\n",
      "Episode 1000/1000, Loss: 0.3003\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No query images available. Please adjust k_query or check the dataset.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[49], line 256\u001b[0m\n\u001b[0;32m    253\u001b[0m k_query_test \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m     \u001b[38;5;66;03m# Number of query samples per class\u001b[39;00m\n\u001b[0;32m    255\u001b[0m threshold \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m  \u001b[38;5;66;03m# Adjust based on validation\u001b[39;00m\n\u001b[1;32m--> 256\u001b[0m \u001b[43mtest_prototypical_network\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_class_to_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    257\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mtest_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_class_to_indices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_way\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_way_test\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    258\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mk_shot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mk_shot_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mk_query_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mthreshold\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    260\u001b[0m \u001b[38;5;66;03m# 15. Explainability Module (Using Guided Backpropagation)\u001b[39;00m\n\u001b[0;32m    261\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mGuidedBackpropReLUModel\u001b[39;00m(nn\u001b[38;5;241m.\u001b[39mModule):\n",
      "Cell \u001b[1;32mIn[49], line 207\u001b[0m, in \u001b[0;36mtest_prototypical_network\u001b[1;34m(model, train_dataset, train_class_to_indices, test_dataset, test_class_to_indices, n_way, k_shot, k_query, threshold)\u001b[0m\n\u001b[0;32m    204\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m    206\u001b[0m     \u001b[38;5;66;03m# Create support set from training dataset\u001b[39;00m\n\u001b[1;32m--> 207\u001b[0m     support_images, support_labels, _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_episode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    208\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_class_to_indices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_way\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk_shot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    209\u001b[0m     support_images \u001b[38;5;241m=\u001b[39m support_images\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m    210\u001b[0m     support_labels \u001b[38;5;241m=\u001b[39m support_labels\u001b[38;5;241m.\u001b[39mto(device)\n",
      "Cell \u001b[1;32mIn[49], line 149\u001b[0m, in \u001b[0;36mcreate_episode\u001b[1;34m(dataset, class_to_indices, n_way, k_shot, k_query)\u001b[0m\n\u001b[0;32m    146\u001b[0m         query_labels\u001b[38;5;241m.\u001b[39mappend(class_to_idx[\u001b[38;5;28mcls\u001b[39m])\n\u001b[0;32m    148\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(query_images) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 149\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo query images available. Please adjust k_query or check the dataset.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(support_images) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    152\u001b[0m     support_images \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(support_images)\n",
      "\u001b[1;31mValueError\u001b[0m: No query images available. Please adjust k_query or check the dataset."
     ]
    }
   ],
   "source": [
    "# 1. Import Libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms, datasets\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "# 2. Set Device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# 3. Prepare the Omniglot Dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.Resize((28, 28)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "train_dataset = datasets.Omniglot(\n",
    "    root='./data',\n",
    "    background=True,\n",
    "    transform=transform,\n",
    "    download=True\n",
    ")\n",
    "\n",
    "test_dataset = datasets.Omniglot(\n",
    "    root='./data',\n",
    "    background=False,\n",
    "    transform=transform,\n",
    "    download=True\n",
    ")\n",
    "\n",
    "# 4. Extract Labels and Build Class-to-Indices Mapping\n",
    "def extract_labels_and_build_mapping(dataset):\n",
    "    labels = []\n",
    "    for idx in range(len(dataset)):\n",
    "        _, label = dataset[idx]\n",
    "        labels.append(label)\n",
    "    class_to_indices = defaultdict(list)\n",
    "    for idx, label in enumerate(labels):\n",
    "        class_to_indices[label].append(idx)\n",
    "    return labels, class_to_indices\n",
    "\n",
    "# Extract labels and mappings for train and test datasets\n",
    "train_labels, train_class_to_indices = extract_labels_and_build_mapping(train_dataset)\n",
    "test_labels, test_class_to_indices = extract_labels_and_build_mapping(test_dataset)\n",
    "\n",
    "# 5. Define the Capsule Network\n",
    "class CapsuleNet(nn.Module):\n",
    "    def __init__(self, output_dim):\n",
    "        super(CapsuleNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 256, kernel_size=9, stride=1)\n",
    "        self.primary_caps = nn.Conv2d(256, 8 * 32, kernel_size=9, stride=2)\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.primary_caps(x)\n",
    "        batch_size = x.size(0)\n",
    "        x = x.view(batch_size, -1, 8)\n",
    "        x = self.squash(x)\n",
    "        x = x.view(batch_size, -1)\n",
    "        return x[:, :self.output_dim]\n",
    "\n",
    "    def squash(self, x):\n",
    "        s_squared_norm = (x ** 2).sum(dim=-1, keepdim=True)\n",
    "        scale = s_squared_norm / (1 + s_squared_norm)\n",
    "        return scale * x / torch.sqrt(s_squared_norm + 1e-8)\n",
    "\n",
    "# 6. Define the Prototypical Network\n",
    "class PrototypicalNetwork(nn.Module):\n",
    "    def __init__(self, feature_extractor):\n",
    "        super(PrototypicalNetwork, self).__init__()\n",
    "        self.feature_extractor = feature_extractor\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeddings = self.feature_extractor(x)\n",
    "        return embeddings\n",
    "\n",
    "# 7. Helper Functions\n",
    "def compute_prototypes(embeddings, labels, n_way, k_shot):\n",
    "    prototypes = []\n",
    "    for i in range(n_way):\n",
    "        class_embeddings = embeddings[labels == i]\n",
    "        class_prototype = class_embeddings.mean(0)\n",
    "        prototypes.append(class_prototype)\n",
    "    prototypes = torch.stack(prototypes)\n",
    "    return prototypes\n",
    "\n",
    "def prototypical_loss(query_embeddings, query_labels, prototypes):\n",
    "    distances = euclidean_distances(query_embeddings, prototypes)\n",
    "    log_p_y = F.log_softmax(-distances, dim=1)\n",
    "    loss = -log_p_y.gather(1, query_labels.unsqueeze(1)).mean()\n",
    "    return loss\n",
    "\n",
    "def euclidean_distances(a, b):\n",
    "    n = a.size(0)\n",
    "    m = b.size(0)\n",
    "    a = a.unsqueeze(1).expand(n, m, -1)\n",
    "    b = b.unsqueeze(0).expand(n, m, -1)\n",
    "    distances = ((a - b) ** 2).sum(2)\n",
    "    return distances\n",
    "\n",
    "def open_set_recognition(embeddings, prototypes, threshold):\n",
    "    distances = euclidean_distances(embeddings, prototypes)\n",
    "    min_distances, predicted_labels = distances.min(1)\n",
    "    is_known = min_distances <= threshold\n",
    "    predicted_labels[~is_known] = -1  # Assign -1 to unknown classes\n",
    "    return predicted_labels, min_distances\n",
    "\n",
    "# 8. Create Episode Function\n",
    "def create_episode(dataset, class_to_indices, n_way, k_shot, k_query):\n",
    "    # Filter out classes that don't have enough samples\n",
    "    classes_with_enough_samples = [\n",
    "        cls for cls, idxs in class_to_indices.items() if len(idxs) >= k_shot + k_query\n",
    "    ]\n",
    "    \n",
    "    if len(classes_with_enough_samples) < n_way:\n",
    "        raise ValueError(f\"Not enough classes with at least {k_shot + k_query} samples. \"\n",
    "                         f\"Available: {len(classes_with_enough_samples)}, Required: {n_way}\")\n",
    "    \n",
    "    selected_classes = random.sample(classes_with_enough_samples, n_way)\n",
    "    support_images = []\n",
    "    query_images = []\n",
    "    support_labels = []\n",
    "    query_labels = []\n",
    "    class_to_idx = {cls: idx for idx, cls in enumerate(selected_classes)}\n",
    "    \n",
    "    for cls in selected_classes:\n",
    "        indices = class_to_indices[cls]\n",
    "        selected_indices = random.sample(indices, k_shot + k_query)\n",
    "        support_indices = selected_indices[:k_shot] if k_shot > 0 else []\n",
    "        query_indices = selected_indices[k_shot:] if k_query > 0 else []\n",
    "\n",
    "        for idx in support_indices:\n",
    "            image, _ = dataset[idx]\n",
    "            support_images.append(image)\n",
    "            support_labels.append(class_to_idx[cls])\n",
    "\n",
    "        for idx in query_indices:\n",
    "            image, _ = dataset[idx]\n",
    "            query_images.append(image)\n",
    "            query_labels.append(class_to_idx[cls])\n",
    "\n",
    "    if len(query_images) == 0:\n",
    "        raise ValueError(\"No query images available. Please adjust k_query or check the dataset.\")\n",
    "    \n",
    "    if len(support_images) > 0:\n",
    "        support_images = torch.stack(support_images)\n",
    "        support_labels = torch.tensor(support_labels)\n",
    "    else:\n",
    "        support_images = None\n",
    "        support_labels = None\n",
    "\n",
    "    query_images = torch.stack(query_images)\n",
    "    query_labels = torch.tensor(query_labels)\n",
    "    \n",
    "    return support_images, support_labels, query_images, query_labels\n",
    "\n",
    "# 9. Training Function\n",
    "def train_prototypical_network(model, dataset, optimizer, class_to_indices,\n",
    "                               num_episodes=1000, n_way=5, k_shot=5, k_query=5):\n",
    "    model.train()\n",
    "    for episode in range(num_episodes):\n",
    "        try:\n",
    "            support_images, support_labels, query_images, query_labels = create_episode(\n",
    "                dataset, class_to_indices, n_way, k_shot, k_query)\n",
    "        except ValueError as e:\n",
    "            print(e)\n",
    "            continue  # Skip this episode if not enough classes\n",
    "        \n",
    "        support_images = support_images.to(device)\n",
    "        query_images = query_images.to(device)\n",
    "        support_labels = support_labels.to(device)\n",
    "        query_labels = query_labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Combine support and query images\n",
    "        images = torch.cat([support_images, query_images], dim=0)\n",
    "        embeddings = model(images)\n",
    "\n",
    "        # Split embeddings\n",
    "        support_embeddings = embeddings[:n_way * k_shot]\n",
    "        query_embeddings = embeddings[n_way * k_shot:]\n",
    "\n",
    "        # Compute prototypes\n",
    "        prototypes = compute_prototypes(support_embeddings, support_labels, n_way, k_shot)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = prototypical_loss(query_embeddings, query_labels, prototypes)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (episode + 1) % 100 == 0:\n",
    "            print(f\"Episode {episode+1}/{num_episodes}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "# 10. Testing Function\n",
    "def test_prototypical_network(model, train_dataset, train_class_to_indices, test_dataset, test_class_to_indices,\n",
    "                              n_way=5, k_shot=0, k_query=1, threshold=1.0):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Create support set from training dataset\n",
    "        support_images, support_labels, _, _ = create_episode(\n",
    "            train_dataset, train_class_to_indices, n_way, k_shot=5, k_query=0)\n",
    "        support_images = support_images.to(device)\n",
    "        support_labels = support_labels.to(device)\n",
    "        support_embeddings = model(support_images)\n",
    "        prototypes = compute_prototypes(support_embeddings, support_labels, n_way, k_shot=5)\n",
    "\n",
    "        # Create query set from test dataset (unseen classes)\n",
    "        try:\n",
    "            _, _, query_images, query_labels = create_episode(\n",
    "                test_dataset, test_class_to_indices, n_way, k_shot=0, k_query=k_query)\n",
    "        except ValueError as e:\n",
    "            print(e)\n",
    "            return\n",
    "        query_images = query_images.to(device)\n",
    "        query_labels = query_labels.to(device)\n",
    "\n",
    "        query_embeddings = model(query_images)\n",
    "\n",
    "        # Open-set recognition\n",
    "        predicted_labels, min_distances = open_set_recognition(query_embeddings, prototypes, threshold)\n",
    "\n",
    "        # Map unknown classes to -1\n",
    "        true_labels = torch.full_like(query_labels, -1)\n",
    "        accuracy = (predicted_labels == true_labels).float().mean().item()\n",
    "        print(f\"Test Accuracy (Open-Set Recognition): {accuracy * 100:.2f}%\")\n",
    "\n",
    "# 11. Initialize Model and Optimizer\n",
    "feature_extractor = CapsuleNet(output_dim=64).to(device)\n",
    "model = PrototypicalNetwork(feature_extractor).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 12. Training Parameters\n",
    "num_episodes = 1000\n",
    "n_way = 5\n",
    "k_shot = 5\n",
    "k_query = 5\n",
    "\n",
    "# 13. Train the Model\n",
    "train_prototypical_network(model, train_dataset, optimizer, train_class_to_indices,\n",
    "                           num_episodes, n_way, k_shot, k_query)\n",
    "\n",
    "# 14. Test the Model\n",
    "# Adjust testing parameters\n",
    "n_way_test = 5       # Number of classes per episode\n",
    "k_shot_test = 0      # Number of support samples per class in the test set\n",
    "k_query_test = 1     # Number of query samples per class\n",
    "\n",
    "threshold = 1.0  # Adjust based on validation\n",
    "test_prototypical_network(model, train_dataset, train_class_to_indices,\n",
    "                          test_dataset, test_class_to_indices, n_way=n_way_test,\n",
    "                          k_shot=k_shot_test, k_query=k_query_test, threshold=threshold)\n",
    "\n",
    "# 15. Explainability Module (Using Guided Backpropagation)\n",
    "class GuidedBackpropReLUModel(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super(GuidedBackpropReLUModel, self).__init__()\n",
    "        self.model = model\n",
    "        self.model.eval()\n",
    "        self.update_relus()\n",
    "\n",
    "    def update_relus(self):\n",
    "        def recursive_relu_apply(module_top):\n",
    "            for idx, module in module_top._modules.items():\n",
    "                if isinstance(module, nn.ReLU):\n",
    "                    module_top._modules[idx] = GuidedReLU()\n",
    "                elif len(module._modules) > 0:\n",
    "                    recursive_relu_apply(module)\n",
    "\n",
    "        recursive_relu_apply(self.model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def generate_gradients(self, input_image, target_class):\n",
    "        input_image = input_image.to(device)\n",
    "        input_image.requires_grad = True\n",
    "        output = self.forward(input_image)\n",
    "        self.model.zero_grad()\n",
    "        grad_target_map = torch.zeros_like(output)\n",
    "        grad_target_map[0][target_class] = 1\n",
    "        output.backward(grad_target_map)\n",
    "        gradients_as_arr = input_image.grad.data.cpu().numpy()[0]\n",
    "        return gradients_as_arr\n",
    "\n",
    "class GuidedReLU(nn.Module):\n",
    "    def forward(self, input):\n",
    "        positive_mask = (input > 0).type_as(input)\n",
    "        output = F.relu(input)\n",
    "        return output * positive_mask\n",
    "\n",
    "# 16. Inference with Explainability\n",
    "def inference_with_explainability(model, image, prototypes, threshold, explainer):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        embedding = model(image.to(device))\n",
    "    predicted_labels, distances = open_set_recognition(embedding, prototypes, threshold)\n",
    "    if predicted_labels.item() == -1:\n",
    "        result = \"Unknown\"\n",
    "        target_class = distances.argmin().item()\n",
    "    else:\n",
    "        result = f\"Class {predicted_labels.item()}\"\n",
    "        target_class = predicted_labels.item()\n",
    "    # Generate Saliency Map\n",
    "    saliency_map = explainer.generate_gradients(image, target_class)\n",
    "    saliency_map = np.transpose(saliency_map, (1, 2, 0))\n",
    "    saliency_map = np.mean(np.abs(saliency_map), axis=2)\n",
    "    return result, saliency_map\n",
    "\n",
    "# 17. Run Inference on a Test Image\n",
    "# Initialize the explainer\n",
    "explainer = GuidedBackpropReLUModel(model.feature_extractor)\n",
    "\n",
    "# Select a test image\n",
    "test_image, _ = test_dataset[0]\n",
    "test_image = test_image.unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "# Run inference\n",
    "result, saliency_map = inference_with_explainability(model, test_image, prototypes,\n",
    "                                                     threshold=threshold, explainer=explainer)\n",
    "print(\"Inference Result:\", result)\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(test_image.squeeze().cpu().numpy(), cmap='gray')\n",
    "plt.imshow(saliency_map, cmap='hot', alpha=0.5)\n",
    "plt.title(\"Saliency Map\")\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "38b5cf5d-792c-43c1-80d1-90b6ea77636e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Episode 100/1000, Loss: 0.3955\n",
      "Episode 200/1000, Loss: 0.6674\n",
      "Episode 300/1000, Loss: 0.5011\n",
      "Episode 400/1000, Loss: 0.3490\n",
      "Episode 500/1000, Loss: 0.6273\n",
      "Episode 600/1000, Loss: 0.5765\n",
      "Episode 700/1000, Loss: 0.8294\n",
      "Episode 800/1000, Loss: 0.3173\n",
      "Episode 900/1000, Loss: 0.6094\n",
      "Episode 1000/1000, Loss: 0.0946\n",
      "Test Accuracy (Open-Set Recognition): 100.00%\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'prototypes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[50], line 334\u001b[0m\n\u001b[0;32m    331\u001b[0m test_image \u001b[38;5;241m=\u001b[39m test_image\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# Add batch dimension\u001b[39;00m\n\u001b[0;32m    333\u001b[0m \u001b[38;5;66;03m# Run inference\u001b[39;00m\n\u001b[1;32m--> 334\u001b[0m result, saliency_map \u001b[38;5;241m=\u001b[39m inference_with_explainability(model, test_image, \u001b[43mprototypes\u001b[49m,\n\u001b[0;32m    335\u001b[0m                                                      threshold\u001b[38;5;241m=\u001b[39mthreshold, explainer\u001b[38;5;241m=\u001b[39mexplainer)\n\u001b[0;32m    336\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInference Result:\u001b[39m\u001b[38;5;124m\"\u001b[39m, result)\n\u001b[0;32m    338\u001b[0m \u001b[38;5;66;03m# Visualization\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'prototypes' is not defined"
     ]
    }
   ],
   "source": [
    "# 1. Import Libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms, datasets\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "# 2. Set Device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# 3. Prepare the Omniglot Dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.Resize((28, 28)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "train_dataset = datasets.Omniglot(\n",
    "    root='./data',\n",
    "    background=True,\n",
    "    transform=transform,\n",
    "    download=True\n",
    ")\n",
    "\n",
    "test_dataset = datasets.Omniglot(\n",
    "    root='./data',\n",
    "    background=False,\n",
    "    transform=transform,\n",
    "    download=True\n",
    ")\n",
    "\n",
    "# 4. Extract Labels and Build Class-to-Indices Mapping\n",
    "def extract_labels_and_build_mapping(dataset):\n",
    "    labels = []\n",
    "    for idx in range(len(dataset)):\n",
    "        _, label = dataset[idx]\n",
    "        labels.append(label)\n",
    "    class_to_indices = defaultdict(list)\n",
    "    for idx, label in enumerate(labels):\n",
    "        class_to_indices[label].append(idx)\n",
    "    return labels, class_to_indices\n",
    "\n",
    "# Extract labels and mappings for train and test datasets\n",
    "train_labels, train_class_to_indices = extract_labels_and_build_mapping(train_dataset)\n",
    "test_labels, test_class_to_indices = extract_labels_and_build_mapping(test_dataset)\n",
    "\n",
    "# 5. Define the Capsule Network\n",
    "class CapsuleNet(nn.Module):\n",
    "    def __init__(self, output_dim):\n",
    "        super(CapsuleNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 256, kernel_size=9, stride=1)\n",
    "        self.primary_caps = nn.Conv2d(256, 8 * 32, kernel_size=9, stride=2)\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.primary_caps(x)\n",
    "        batch_size = x.size(0)\n",
    "        x = x.view(batch_size, -1, 8)\n",
    "        x = self.squash(x)\n",
    "        x = x.view(batch_size, -1)\n",
    "        return x[:, :self.output_dim]\n",
    "\n",
    "    def squash(self, x):\n",
    "        s_squared_norm = (x ** 2).sum(dim=-1, keepdim=True)\n",
    "        scale = s_squared_norm / (1 + s_squared_norm)\n",
    "        return scale * x / torch.sqrt(s_squared_norm + 1e-8)\n",
    "\n",
    "# 6. Define the Prototypical Network\n",
    "class PrototypicalNetwork(nn.Module):\n",
    "    def __init__(self, feature_extractor):\n",
    "        super(PrototypicalNetwork, self).__init__()\n",
    "        self.feature_extractor = feature_extractor\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeddings = self.feature_extractor(x)\n",
    "        return embeddings\n",
    "\n",
    "# 7. Helper Functions\n",
    "def compute_prototypes(embeddings, labels, n_way, k_shot):\n",
    "    prototypes = []\n",
    "    for i in range(n_way):\n",
    "        class_embeddings = embeddings[labels == i]\n",
    "        class_prototype = class_embeddings.mean(0)\n",
    "        prototypes.append(class_prototype)\n",
    "    prototypes = torch.stack(prototypes)\n",
    "    return prototypes\n",
    "\n",
    "def prototypical_loss(query_embeddings, query_labels, prototypes):\n",
    "    distances = euclidean_distances(query_embeddings, prototypes)\n",
    "    log_p_y = F.log_softmax(-distances, dim=1)\n",
    "    loss = -log_p_y.gather(1, query_labels.unsqueeze(1)).mean()\n",
    "    return loss\n",
    "\n",
    "def euclidean_distances(a, b):\n",
    "    n = a.size(0)\n",
    "    m = b.size(0)\n",
    "    a = a.unsqueeze(1).expand(n, m, -1)\n",
    "    b = b.unsqueeze(0).expand(n, m, -1)\n",
    "    distances = ((a - b) ** 2).sum(2)\n",
    "    return distances\n",
    "\n",
    "def open_set_recognition(embeddings, prototypes, threshold):\n",
    "    distances = euclidean_distances(embeddings, prototypes)\n",
    "    min_distances, predicted_labels = distances.min(1)\n",
    "    is_known = min_distances <= threshold\n",
    "    predicted_labels[~is_known] = -1  # Assign -1 to unknown classes\n",
    "    return predicted_labels, min_distances\n",
    "\n",
    "# 8. Create Episode Function (Updated)\n",
    "def create_episode(dataset, class_to_indices, n_way, k_shot, k_query):\n",
    "    # Filter out classes that don't have enough samples\n",
    "    classes_with_enough_samples = [\n",
    "        cls for cls, idxs in class_to_indices.items() if len(idxs) >= k_shot + k_query\n",
    "    ]\n",
    "    \n",
    "    if len(classes_with_enough_samples) < n_way:\n",
    "        raise ValueError(f\"Not enough classes with at least {k_shot + k_query} samples. \"\n",
    "                         f\"Available: {len(classes_with_enough_samples)}, Required: {n_way}\")\n",
    "    \n",
    "    selected_classes = random.sample(classes_with_enough_samples, n_way)\n",
    "    support_images = []\n",
    "    query_images = []\n",
    "    support_labels = []\n",
    "    query_labels = []\n",
    "    class_to_idx = {cls: idx for idx, cls in enumerate(selected_classes)}\n",
    "    \n",
    "    for cls in selected_classes:\n",
    "        indices = class_to_indices[cls]\n",
    "        selected_indices = random.sample(indices, k_shot + k_query)\n",
    "        support_indices = selected_indices[:k_shot] if k_shot > 0 else []\n",
    "        query_indices = selected_indices[k_shot:] if k_query > 0 else []\n",
    "\n",
    "        for idx in support_indices:\n",
    "            image, _ = dataset[idx]\n",
    "            support_images.append(image)\n",
    "            support_labels.append(class_to_idx[cls])\n",
    "\n",
    "        for idx in query_indices:\n",
    "            image, _ = dataset[idx]\n",
    "            query_images.append(image)\n",
    "            query_labels.append(class_to_idx[cls])\n",
    "\n",
    "    # Only raise an error if k_query > 0 and no query images\n",
    "    if k_query > 0 and len(query_images) == 0:\n",
    "        raise ValueError(\"No query images available. Please adjust k_query or check the dataset.\")\n",
    "\n",
    "    # Only raise an error if k_shot > 0 and no support images\n",
    "    if k_shot > 0 and len(support_images) == 0:\n",
    "        raise ValueError(\"No support images available. Please adjust k_shot or check the dataset.\")\n",
    "\n",
    "    if len(support_images) > 0:\n",
    "        support_images = torch.stack(support_images)\n",
    "        support_labels = torch.tensor(support_labels)\n",
    "    else:\n",
    "        support_images = None\n",
    "        support_labels = None\n",
    "\n",
    "    if len(query_images) > 0:\n",
    "        query_images = torch.stack(query_images)\n",
    "        query_labels = torch.tensor(query_labels)\n",
    "    else:\n",
    "        query_images = None\n",
    "        query_labels = None\n",
    "\n",
    "    return support_images, support_labels, query_images, query_labels\n",
    "\n",
    "# 9. Training Function\n",
    "def train_prototypical_network(model, dataset, optimizer, class_to_indices,\n",
    "                               num_episodes=1000, n_way=5, k_shot=5, k_query=5):\n",
    "    model.train()\n",
    "    for episode in range(num_episodes):\n",
    "        try:\n",
    "            support_images, support_labels, query_images, query_labels = create_episode(\n",
    "                dataset, class_to_indices, n_way, k_shot, k_query)\n",
    "        except ValueError as e:\n",
    "            print(e)\n",
    "            continue  # Skip this episode if not enough classes\n",
    "        \n",
    "        support_images = support_images.to(device)\n",
    "        query_images = query_images.to(device)\n",
    "        support_labels = support_labels.to(device)\n",
    "        query_labels = query_labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Combine support and query images\n",
    "        images = torch.cat([support_images, query_images], dim=0)\n",
    "        embeddings = model(images)\n",
    "\n",
    "        # Split embeddings\n",
    "        support_embeddings = embeddings[:n_way * k_shot]\n",
    "        query_embeddings = embeddings[n_way * k_shot:]\n",
    "\n",
    "        # Compute prototypes\n",
    "        prototypes = compute_prototypes(support_embeddings, support_labels, n_way, k_shot)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = prototypical_loss(query_embeddings, query_labels, prototypes)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (episode + 1) % 100 == 0:\n",
    "            print(f\"Episode {episode+1}/{num_episodes}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "# 10. Testing Function\n",
    "def test_prototypical_network(model, train_dataset, train_class_to_indices, test_dataset, test_class_to_indices,\n",
    "                              n_way=5, k_shot=0, k_query=1, threshold=1.0):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Create support set from training dataset\n",
    "        support_images, support_labels, _, _ = create_episode(\n",
    "            train_dataset, train_class_to_indices, n_way, k_shot=5, k_query=0)\n",
    "        support_images = support_images.to(device)\n",
    "        support_labels = support_labels.to(device)\n",
    "        support_embeddings = model(support_images)\n",
    "        prototypes = compute_prototypes(support_embeddings, support_labels, n_way, k_shot=5)\n",
    "\n",
    "        # Create query set from test dataset (unseen classes)\n",
    "        try:\n",
    "            _, _, query_images, query_labels = create_episode(\n",
    "                test_dataset, test_class_to_indices, n_way, k_shot=0, k_query=k_query)\n",
    "        except ValueError as e:\n",
    "            print(e)\n",
    "            return\n",
    "        query_images = query_images.to(device)\n",
    "        query_labels = query_labels.to(device)\n",
    "\n",
    "        query_embeddings = model(query_images)\n",
    "\n",
    "        # Open-set recognition\n",
    "        predicted_labels, min_distances = open_set_recognition(query_embeddings, prototypes, threshold)\n",
    "\n",
    "        # Map unknown classes to -1\n",
    "        true_labels = torch.full_like(query_labels, -1)\n",
    "        accuracy = (predicted_labels == true_labels).float().mean().item()\n",
    "        print(f\"Test Accuracy (Open-Set Recognition): {accuracy * 100:.2f}%\")\n",
    "\n",
    "# 11. Initialize Model and Optimizer\n",
    "feature_extractor = CapsuleNet(output_dim=64).to(device)\n",
    "model = PrototypicalNetwork(feature_extractor).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 12. Training Parameters\n",
    "num_episodes = 1000\n",
    "n_way = 5\n",
    "k_shot = 5\n",
    "k_query = 5\n",
    "\n",
    "# 13. Train the Model\n",
    "train_prototypical_network(model, train_dataset, optimizer, train_class_to_indices,\n",
    "                           num_episodes, n_way, k_shot, k_query)\n",
    "\n",
    "# 14. Test the Model\n",
    "# Adjust testing parameters\n",
    "n_way_test = 5       # Number of classes per episode\n",
    "k_shot_test = 0      # Number of support samples per class in the test set\n",
    "k_query_test = 1     # Number of query samples per class\n",
    "\n",
    "threshold = 1.0  # Adjust based on validation\n",
    "test_prototypical_network(model, train_dataset, train_class_to_indices,\n",
    "                          test_dataset, test_class_to_indices, n_way=n_way_test,\n",
    "                          k_shot=k_shot_test, k_query=k_query_test, threshold=threshold)\n",
    "\n",
    "# 15. Explainability Module (Using Guided Backpropagation)\n",
    "class GuidedBackpropReLUModel(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super(GuidedBackpropReLUModel, self).__init__()\n",
    "        self.model = model\n",
    "        self.model.eval()\n",
    "        self.update_relus()\n",
    "\n",
    "    def update_relus(self):\n",
    "        def recursive_relu_apply(module_top):\n",
    "            for idx, module in module_top._modules.items():\n",
    "                if isinstance(module, nn.ReLU):\n",
    "                    module_top._modules[idx] = GuidedReLU()\n",
    "                elif len(module._modules) > 0:\n",
    "                    recursive_relu_apply(module)\n",
    "\n",
    "        recursive_relu_apply(self.model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def generate_gradients(self, input_image, target_class):\n",
    "        input_image = input_image.to(device)\n",
    "        input_image.requires_grad = True\n",
    "        output = self.forward(input_image)\n",
    "        self.model.zero_grad()\n",
    "        grad_target_map = torch.zeros_like(output)\n",
    "        grad_target_map[0][target_class] = 1\n",
    "        output.backward(grad_target_map)\n",
    "        gradients_as_arr = input_image.grad.data.cpu().numpy()[0]\n",
    "        return gradients_as_arr\n",
    "\n",
    "class GuidedReLU(nn.Module):\n",
    "    def forward(self, input):\n",
    "        positive_mask = (input > 0).type_as(input)\n",
    "        output = F.relu(input)\n",
    "        return output * positive_mask\n",
    "\n",
    "# 16. Inference with Explainability\n",
    "def inference_with_explainability(model, image, prototypes, threshold, explainer):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        embedding = model(image.to(device))\n",
    "    predicted_labels, distances = open_set_recognition(embedding, prototypes, threshold)\n",
    "    if predicted_labels.item() == -1:\n",
    "        result = \"Unknown\"\n",
    "        target_class = distances.argmin().item()\n",
    "    else:\n",
    "        result = f\"Class {predicted_labels.item()}\"\n",
    "        target_class = predicted_labels.item()\n",
    "    # Generate Saliency Map\n",
    "    saliency_map = explainer.generate_gradients(image, target_class)\n",
    "    saliency_map = np.transpose(saliency_map, (1, 2, 0))\n",
    "    saliency_map = np.mean(np.abs(saliency_map), axis=2)\n",
    "    return result, saliency_map\n",
    "\n",
    "# 17. Run Inference on a Test Image\n",
    "# Initialize the explainer\n",
    "explainer = GuidedBackpropReLUModel(model.feature_extractor)\n",
    "\n",
    "# Select a test image\n",
    "test_image, _ = test_dataset[0]\n",
    "test_image = test_image.unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "# Run inference\n",
    "result, saliency_map = inference_with_explainability(model, test_image, prototypes,\n",
    "                                                     threshold=threshold, explainer=explainer)\n",
    "print(\"Inference Result:\", result)\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(test_image.squeeze().cpu().numpy(), cmap='gray')\n",
    "plt.imshow(saliency_map, cmap='hot', alpha=0.5)\n",
    "plt.title(\"Saliency Map\")\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b3cedae8-c641-49fd-860d-58e336ada294",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Episode 100/1000, Loss: 0.4596\n",
      "Episode 200/1000, Loss: 0.9263\n",
      "Episode 300/1000, Loss: 0.3745\n",
      "Episode 400/1000, Loss: 0.6505\n",
      "Episode 500/1000, Loss: 0.1231\n",
      "Episode 600/1000, Loss: 0.3348\n",
      "Episode 700/1000, Loss: 0.4125\n",
      "Episode 800/1000, Loss: 0.4413\n",
      "Episode 900/1000, Loss: 0.6400\n",
      "Episode 1000/1000, Loss: 0.4430\n",
      "Test Accuracy (Open-Set Recognition): 100.00%\n",
      "Inference Result: Unknown\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[51], line 294\u001b[0m\n\u001b[0;32m    292\u001b[0m \u001b[38;5;66;03m# Visualization\u001b[39;00m\n\u001b[0;32m    293\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m8\u001b[39m))\n\u001b[1;32m--> 294\u001b[0m plt\u001b[38;5;241m.\u001b[39mimshow(\u001b[43mtest_image\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, cmap\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgray\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    295\u001b[0m plt\u001b[38;5;241m.\u001b[39mimshow(saliency_map, cmap\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhot\u001b[39m\u001b[38;5;124m'\u001b[39m, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m)\n\u001b[0;32m    296\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSaliency Map\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead."
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 1. Import Libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms, datasets\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "# 2. Set Device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# 3. Prepare the Omniglot Dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.Resize((28, 28)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "train_dataset = datasets.Omniglot(\n",
    "    root='./data',\n",
    "    background=True,\n",
    "    transform=transform,\n",
    "    download=True\n",
    ")\n",
    "\n",
    "test_dataset = datasets.Omniglot(\n",
    "    root='./data',\n",
    "    background=False,\n",
    "    transform=transform,\n",
    "    download=True\n",
    ")\n",
    "\n",
    "# 4. Extract Labels and Build Class-to-Indices Mapping\n",
    "def extract_labels_and_build_mapping(dataset):\n",
    "    labels = []\n",
    "    for idx in range(len(dataset)):\n",
    "        _, label = dataset[idx]\n",
    "        labels.append(label)\n",
    "    class_to_indices = defaultdict(list)\n",
    "    for idx, label in enumerate(labels):\n",
    "        class_to_indices[label].append(idx)\n",
    "    return labels, class_to_indices\n",
    "\n",
    "# Extract labels and mappings for train and test datasets\n",
    "train_labels, train_class_to_indices = extract_labels_and_build_mapping(train_dataset)\n",
    "test_labels, test_class_to_indices = extract_labels_and_build_mapping(test_dataset)\n",
    "\n",
    "# 5. Define the Capsule Network\n",
    "class CapsuleNet(nn.Module):\n",
    "    def __init__(self, output_dim):\n",
    "        super(CapsuleNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 256, kernel_size=9, stride=1)\n",
    "        self.primary_caps = nn.Conv2d(256, 8 * 32, kernel_size=9, stride=2)\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.primary_caps(x)\n",
    "        batch_size = x.size(0)\n",
    "        x = x.view(batch_size, -1, 8)\n",
    "        x = self.squash(x)\n",
    "        x = x.view(batch_size, -1)\n",
    "        return x[:, :self.output_dim]\n",
    "\n",
    "    def squash(self, x):\n",
    "        s_squared_norm = (x ** 2).sum(dim=-1, keepdim=True)\n",
    "        scale = s_squared_norm / (1 + s_squared_norm)\n",
    "        return scale * x / torch.sqrt(s_squared_norm + 1e-8)\n",
    "\n",
    "# 6. Define the Prototypical Network\n",
    "class PrototypicalNetwork(nn.Module):\n",
    "    def __init__(self, feature_extractor):\n",
    "        super(PrototypicalNetwork, self).__init__()\n",
    "        self.feature_extractor = feature_extractor\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeddings = self.feature_extractor(x)\n",
    "        return embeddings\n",
    "\n",
    "# 7. Helper Functions\n",
    "def compute_prototypes(embeddings, labels, n_way, k_shot):\n",
    "    prototypes = []\n",
    "    for i in range(n_way):\n",
    "        class_embeddings = embeddings[labels == i]\n",
    "        class_prototype = class_embeddings.mean(0)\n",
    "        prototypes.append(class_prototype)\n",
    "    prototypes = torch.stack(prototypes)\n",
    "    return prototypes\n",
    "\n",
    "def prototypical_loss(query_embeddings, query_labels, prototypes):\n",
    "    distances = euclidean_distances(query_embeddings, prototypes)\n",
    "    log_p_y = F.log_softmax(-distances, dim=1)\n",
    "    loss = -log_p_y.gather(1, query_labels.unsqueeze(1)).mean()\n",
    "    return loss\n",
    "\n",
    "def euclidean_distances(a, b):\n",
    "    n = a.size(0)\n",
    "    m = b.size(0)\n",
    "    a = a.unsqueeze(1).expand(n, m, -1)\n",
    "    b = b.unsqueeze(0).expand(n, m, -1)\n",
    "    distances = ((a - b) ** 2).sum(2)\n",
    "    return distances\n",
    "\n",
    "def open_set_recognition(embeddings, prototypes, threshold):\n",
    "    distances = euclidean_distances(embeddings, prototypes)\n",
    "    min_distances, predicted_labels = distances.min(1)\n",
    "    is_known = min_distances <= threshold\n",
    "    predicted_labels[~is_known] = -1  # Assign -1 to unknown classes\n",
    "    return predicted_labels, min_distances\n",
    "\n",
    "# 8. Create Episode Function (Updated)\n",
    "def create_episode(dataset, class_to_indices, n_way, k_shot, k_query):\n",
    "    # Filter out classes that don't have enough samples\n",
    "    classes_with_enough_samples = [\n",
    "        cls for cls, idxs in class_to_indices.items() if len(idxs) >= k_shot + k_query\n",
    "    ]\n",
    "    \n",
    "    if len(classes_with_enough_samples) < n_way:\n",
    "        raise ValueError(f\"Not enough classes with at least {k_shot + k_query} samples. \"\n",
    "                         f\"Available: {len(classes_with_enough_samples)}, Required: {n_way}\")\n",
    "    \n",
    "    selected_classes = random.sample(classes_with_enough_samples, n_way)\n",
    "    support_images = []\n",
    "    query_images = []\n",
    "    support_labels = []\n",
    "    query_labels = []\n",
    "    class_to_idx = {cls: idx for idx, cls in enumerate(selected_classes)}\n",
    "    \n",
    "    for cls in selected_classes:\n",
    "        indices = class_to_indices[cls]\n",
    "        selected_indices = random.sample(indices, k_shot + k_query)\n",
    "        support_indices = selected_indices[:k_shot] if k_shot > 0 else []\n",
    "        query_indices = selected_indices[k_shot:] if k_query > 0 else []\n",
    "\n",
    "        for idx in support_indices:\n",
    "            image, _ = dataset[idx]\n",
    "            support_images.append(image)\n",
    "            support_labels.append(class_to_idx[cls])\n",
    "\n",
    "        for idx in query_indices:\n",
    "            image, _ = dataset[idx]\n",
    "            query_images.append(image)\n",
    "            query_labels.append(class_to_idx[cls])\n",
    "\n",
    "    # Only raise an error if k_query > 0 and no query images\n",
    "    if k_query > 0 and len(query_images) == 0:\n",
    "        raise ValueError(\"No query images available. Please adjust k_query or check the dataset.\")\n",
    "\n",
    "    # Only raise an error if k_shot > 0 and no support images\n",
    "    if k_shot > 0 and len(support_images) == 0:\n",
    "        raise ValueError(\"No support images available. Please adjust k_shot or check the dataset.\")\n",
    "\n",
    "    if len(support_images) > 0:\n",
    "        support_images = torch.stack(support_images)\n",
    "        support_labels = torch.tensor(support_labels)\n",
    "    else:\n",
    "        support_images = None\n",
    "        support_labels = None\n",
    "\n",
    "    if len(query_images) > 0:\n",
    "        query_images = torch.stack(query_images)\n",
    "        query_labels = torch.tensor(query_labels)\n",
    "    else:\n",
    "        query_images = None\n",
    "        query_labels = None\n",
    "\n",
    "    return support_images, support_labels, query_images, query_labels\n",
    "\n",
    "# 9. Training Function\n",
    "def train_prototypical_network(model, dataset, optimizer, class_to_indices,\n",
    "                               num_episodes=1000, n_way=5, k_shot=5, k_query=5):\n",
    "    model.train()\n",
    "    for episode in range(num_episodes):\n",
    "        try:\n",
    "            support_images, support_labels, query_images, query_labels = create_episode(\n",
    "                dataset, class_to_indices, n_way, k_shot, k_query)\n",
    "        except ValueError as e:\n",
    "            print(e)\n",
    "            continue  # Skip this episode if not enough classes\n",
    "        \n",
    "        support_images = support_images.to(device)\n",
    "        query_images = query_images.to(device)\n",
    "        support_labels = support_labels.to(device)\n",
    "        query_labels = query_labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Combine support and query images\n",
    "        images = torch.cat([support_images, query_images], dim=0)\n",
    "        embeddings = model(images)\n",
    "\n",
    "        # Split embeddings\n",
    "        support_embeddings = embeddings[:n_way * k_shot]\n",
    "        query_embeddings = embeddings[n_way * k_shot:]\n",
    "\n",
    "        # Compute prototypes\n",
    "        prototypes = compute_prototypes(support_embeddings, support_labels, n_way, k_shot)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = prototypical_loss(query_embeddings, query_labels, prototypes)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (episode + 1) % 100 == 0:\n",
    "            print(f\"Episode {episode+1}/{num_episodes}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "# 10. Testing Function\n",
    "def test_prototypical_network(model, train_dataset, train_class_to_indices, test_dataset, test_class_to_indices,\n",
    "                              n_way=5, k_shot=0, k_query=1, threshold=1.0):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Create support set from training dataset\n",
    "        support_images, support_labels, _, _ = create_episode(\n",
    "            train_dataset, train_class_to_indices, n_way, k_shot=5, k_query=0)\n",
    "        support_images = support_images.to(device)\n",
    "        support_labels = support_labels.to(device)\n",
    "        support_embeddings = model(support_images)\n",
    "        prototypes = compute_prototypes(support_embeddings, support_labels, n_way, k_shot=5)\n",
    "\n",
    "        # Create query set from test dataset (unseen classes)\n",
    "        try:\n",
    "            _, _, query_images, query_labels = create_episode(\n",
    "                test_dataset, test_class_to_indices, n_way, k_shot=0, k_query=k_query)\n",
    "        except ValueError as e:\n",
    "            print(e)\n",
    "            return\n",
    "        query_images = query_images.to(device)\n",
    "        query_labels = query_labels.to(device)\n",
    "\n",
    "        query_embeddings = model(query_images)\n",
    "\n",
    "        # Open-set recognition\n",
    "        predicted_labels, min_distances = open_set_recognition(query_embeddings, prototypes, threshold)\n",
    "\n",
    "        # Map unknown classes to -1\n",
    "        true_labels = torch.full_like(query_labels, -1)\n",
    "        accuracy = (predicted_labels == true_labels).float().mean().item()\n",
    "        print(f\"Test Accuracy (Open-Set Recognition): {accuracy * 100:.2f}%\")\n",
    "\n",
    "# 11. Initialize Model and Optimizer\n",
    "feature_extractor = CapsuleNet(output_dim=64).to(device)\n",
    "model = PrototypicalNetwork(feature_extractor).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 12. Training Parameters\n",
    "num_episodes = 1000\n",
    "n_way = 5\n",
    "k_shot = 5\n",
    "k_query = 5\n",
    "\n",
    "# 13. Train the Model\n",
    "train_prototypical_network(model, train_dataset, optimizer, train_class_to_indices,\n",
    "                           num_episodes, n_way, k_shot, k_query)\n",
    "\n",
    "# 14. Test the Model\n",
    "# Adjust testing parameters\n",
    "n_way_test = 5       # Number of classes per episode\n",
    "k_shot_test = 0      # Number of support samples per class in the test set\n",
    "k_query_test = 1     # Number of query samples per class\n",
    "\n",
    "threshold = 1.0  # Adjust based on validation\n",
    "test_prototypical_network(model, train_dataset, train_class_to_indices,\n",
    "                          test_dataset, test_class_to_indices, n_way=n_way_test,\n",
    "                          k_shot=k_shot_test, k_query=k_query_test, threshold=threshold)\n",
    "\n",
    "# 15. Compute Prototypes for Inference\n",
    "with torch.no_grad():\n",
    "    # Use the same n_way and k_shot as during training to compute prototypes\n",
    "    support_images, support_labels, _, _ = create_episode(\n",
    "        train_dataset, train_class_to_indices, n_way=n_way, k_shot=k_shot, k_query=0)\n",
    "    support_images = support_images.to(device)\n",
    "    support_labels = support_labels.to(device)\n",
    "    support_embeddings = model(support_images)\n",
    "    prototypes = compute_prototypes(support_embeddings, support_labels, n_way=n_way, k_shot=k_shot)\n",
    "\n",
    "# 16. Inference with Explainability\n",
    "# Initialize the explainer\n",
    "explainer = GuidedBackpropReLUModel(model.feature_extractor)\n",
    "\n",
    "# Select a test image\n",
    "test_image, _ = test_dataset[0]\n",
    "test_image = test_image.unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "# Run inference\n",
    "result, saliency_map = inference_with_explainability(model, test_image, prototypes,\n",
    "                                                     threshold=threshold, explainer=explainer)\n",
    "print(\"Inference Result:\", result)\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(test_image.squeeze().cpu().numpy(), cmap='gray')\n",
    "plt.imshow(saliency_map, cmap='hot', alpha=0.5)\n",
    "plt.title(\"Saliency Map\")\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ec9ebf0d-a555-4ce4-bfc8-2e5c036b03a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualization\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(test_image.squeeze().detach().cpu().numpy(), cmap='gray')\n",
    "plt.imshow(saliency_map, cmap='hot', alpha=0.5)\n",
    "plt.title(\"Saliency Map\")\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8d7b8931-cfba-434e-9bf4-e239b5518189",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Episode 100/1000, Loss: 0.4584\n",
      "Episode 200/1000, Loss: 0.0982\n",
      "Episode 300/1000, Loss: 0.4216\n",
      "Episode 400/1000, Loss: 0.2051\n",
      "Episode 500/1000, Loss: 0.2990\n",
      "Episode 600/1000, Loss: 0.0433\n",
      "Episode 700/1000, Loss: 0.1824\n",
      "Episode 800/1000, Loss: 0.0809\n",
      "Episode 900/1000, Loss: 0.6285\n",
      "Episode 1000/1000, Loss: 0.0449\n",
      "Test Accuracy (Open-Set Recognition): 96.00%\n",
      "Inference Result: Unknown\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 1. Import Libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms, datasets\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "# 2. Set Device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# 3. Prepare the MNIST Dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((28, 28)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST(\n",
    "    root='./data',\n",
    "    train=True,\n",
    "    transform=transform,\n",
    "    download=True\n",
    ")\n",
    "\n",
    "test_dataset = datasets.MNIST(\n",
    "    root='./data',\n",
    "    train=False,\n",
    "    transform=transform,\n",
    "    download=True\n",
    ")\n",
    "\n",
    "# 4. Extract Labels and Build Class-to-Indices Mapping\n",
    "def extract_labels_and_build_mapping(dataset):\n",
    "    labels = []\n",
    "    for idx in range(len(dataset)):\n",
    "        _, label = dataset[idx]\n",
    "        labels.append(label)\n",
    "    class_to_indices = defaultdict(list)\n",
    "    for idx, label in enumerate(labels):\n",
    "        class_to_indices[label].append(idx)\n",
    "    return labels, class_to_indices\n",
    "\n",
    "# Extract labels and mappings for train and test datasets\n",
    "train_labels, train_class_to_indices = extract_labels_and_build_mapping(train_dataset)\n",
    "test_labels, test_class_to_indices = extract_labels_and_build_mapping(test_dataset)\n",
    "\n",
    "# 5. Define the Capsule Network\n",
    "class CapsuleNet(nn.Module):\n",
    "    def __init__(self, output_dim):\n",
    "        super(CapsuleNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 256, kernel_size=9, stride=1)\n",
    "        self.primary_caps = nn.Conv2d(256, 8 * 32, kernel_size=9, stride=2)\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.primary_caps(x)\n",
    "        batch_size = x.size(0)\n",
    "        x = x.view(batch_size, -1, 8)\n",
    "        x = self.squash(x)\n",
    "        x = x.view(batch_size, -1)\n",
    "        return x[:, :self.output_dim]\n",
    "\n",
    "    def squash(self, x):\n",
    "        s_squared_norm = (x ** 2).sum(dim=-1, keepdim=True)\n",
    "        scale = s_squared_norm / (1 + s_squared_norm)\n",
    "        return scale * x / torch.sqrt(s_squared_norm + 1e-8)\n",
    "\n",
    "# 6. Define the Prototypical Network\n",
    "class PrototypicalNetwork(nn.Module):\n",
    "    def __init__(self, feature_extractor):\n",
    "        super(PrototypicalNetwork, self).__init__()\n",
    "        self.feature_extractor = feature_extractor\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeddings = self.feature_extractor(x)\n",
    "        return embeddings\n",
    "\n",
    "# 7. Helper Functions\n",
    "def compute_prototypes(embeddings, labels, n_way, k_shot):\n",
    "    prototypes = []\n",
    "    for i in range(n_way):\n",
    "        class_embeddings = embeddings[labels == i]\n",
    "        class_prototype = class_embeddings.mean(0)\n",
    "        prototypes.append(class_prototype)\n",
    "    prototypes = torch.stack(prototypes)\n",
    "    return prototypes\n",
    "\n",
    "def prototypical_loss(query_embeddings, query_labels, prototypes):\n",
    "    distances = euclidean_distances(query_embeddings, prototypes)\n",
    "    log_p_y = F.log_softmax(-distances, dim=1)\n",
    "    loss = -log_p_y.gather(1, query_labels.unsqueeze(1)).mean()\n",
    "    return loss\n",
    "\n",
    "def euclidean_distances(a, b):\n",
    "    n = a.size(0)\n",
    "    m = b.size(0)\n",
    "    a = a.unsqueeze(1).expand(n, m, -1)\n",
    "    b = b.unsqueeze(0).expand(n, m, -1)\n",
    "    distances = ((a - b) ** 2).sum(2)\n",
    "    return distances\n",
    "\n",
    "def open_set_recognition(embeddings, prototypes, threshold):\n",
    "    distances = euclidean_distances(embeddings, prototypes)\n",
    "    min_distances, predicted_labels = distances.min(1)\n",
    "    is_known = min_distances <= threshold\n",
    "    predicted_labels[~is_known] = -1  # Assign -1 to unknown classes\n",
    "    return predicted_labels, min_distances\n",
    "\n",
    "# 8. Create Episode Function (Updated)\n",
    "def create_episode(dataset, class_to_indices, n_way, k_shot, k_query):\n",
    "    # Filter out classes that don't have enough samples\n",
    "    classes_with_enough_samples = [\n",
    "        cls for cls, idxs in class_to_indices.items() if len(idxs) >= k_shot + k_query\n",
    "    ]\n",
    "    \n",
    "    if len(classes_with_enough_samples) < n_way:\n",
    "        raise ValueError(f\"Not enough classes with at least {k_shot + k_query} samples. \"\n",
    "                         f\"Available: {len(classes_with_enough_samples)}, Required: {n_way}\")\n",
    "    \n",
    "    selected_classes = random.sample(classes_with_enough_samples, n_way)\n",
    "    support_images = []\n",
    "    query_images = []\n",
    "    support_labels = []\n",
    "    query_labels = []\n",
    "    class_to_idx = {cls: idx for idx, cls in enumerate(selected_classes)}\n",
    "    \n",
    "    for cls in selected_classes:\n",
    "        indices = class_to_indices[cls]\n",
    "        selected_indices = random.sample(indices, k_shot + k_query)\n",
    "        support_indices = selected_indices[:k_shot] if k_shot > 0 else []\n",
    "        query_indices = selected_indices[k_shot:] if k_query > 0 else []\n",
    "\n",
    "        for idx in support_indices:\n",
    "            image, _ = dataset[idx]\n",
    "            support_images.append(image)\n",
    "            support_labels.append(class_to_idx[cls])\n",
    "\n",
    "        for idx in query_indices:\n",
    "            image, _ = dataset[idx]\n",
    "            query_images.append(image)\n",
    "            query_labels.append(class_to_idx[cls])\n",
    "\n",
    "    # Only raise an error if k_query > 0 and no query images\n",
    "    if k_query > 0 and len(query_images) == 0:\n",
    "        raise ValueError(\"No query images available. Please adjust k_query or check the dataset.\")\n",
    "\n",
    "    # Only raise an error if k_shot > 0 and no support images\n",
    "    if k_shot > 0 and len(support_images) == 0:\n",
    "        raise ValueError(\"No support images available. Please adjust k_shot or check the dataset.\")\n",
    "\n",
    "    if len(support_images) > 0:\n",
    "        support_images = torch.stack(support_images)\n",
    "        support_labels = torch.tensor(support_labels)\n",
    "    else:\n",
    "        support_images = None\n",
    "        support_labels = None\n",
    "\n",
    "    if len(query_images) > 0:\n",
    "        query_images = torch.stack(query_images)\n",
    "        query_labels = torch.tensor(query_labels)\n",
    "    else:\n",
    "        query_images = None\n",
    "        query_labels = None\n",
    "\n",
    "    return support_images, support_labels, query_images, query_labels\n",
    "\n",
    "# 9. Training Function\n",
    "def train_prototypical_network(model, dataset, optimizer, class_to_indices,\n",
    "                               num_episodes=1000, n_way=5, k_shot=5, k_query=5):\n",
    "    model.train()\n",
    "    for episode in range(num_episodes):\n",
    "        try:\n",
    "            support_images, support_labels, query_images, query_labels = create_episode(\n",
    "                dataset, class_to_indices, n_way, k_shot, k_query)\n",
    "        except ValueError as e:\n",
    "            print(e)\n",
    "            continue  # Skip this episode if not enough classes\n",
    "        \n",
    "        support_images = support_images.to(device)\n",
    "        query_images = query_images.to(device)\n",
    "        support_labels = support_labels.to(device)\n",
    "        query_labels = query_labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Combine support and query images\n",
    "        images = torch.cat([support_images, query_images], dim=0)\n",
    "        embeddings = model(images)\n",
    "\n",
    "        # Split embeddings\n",
    "        support_embeddings = embeddings[:n_way * k_shot]\n",
    "        query_embeddings = embeddings[n_way * k_shot:]\n",
    "\n",
    "        # Compute prototypes\n",
    "        prototypes = compute_prototypes(support_embeddings, support_labels, n_way, k_shot)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = prototypical_loss(query_embeddings, query_labels, prototypes)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (episode + 1) % 100 == 0:\n",
    "            print(f\"Episode {episode+1}/{num_episodes}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "# 10. Testing Function\n",
    "def test_prototypical_network(model, train_dataset, train_class_to_indices, test_dataset, test_class_to_indices,\n",
    "                              n_way=5, k_shot=0, k_query=5, threshold=1.0):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Create support set from training dataset\n",
    "        support_images, support_labels, _, _ = create_episode(\n",
    "            train_dataset, train_class_to_indices, n_way, k_shot=5, k_query=0)\n",
    "        support_images = support_images.to(device)\n",
    "        support_labels = support_labels.to(device)\n",
    "        support_embeddings = model(support_images)\n",
    "        prototypes = compute_prototypes(support_embeddings, support_labels, n_way, k_shot=5)\n",
    "\n",
    "        # Create query set from test dataset (unseen classes)\n",
    "        try:\n",
    "            _, _, query_images, query_labels = create_episode(\n",
    "                test_dataset, test_class_to_indices, n_way, k_shot=0, k_query=k_query)\n",
    "        except ValueError as e:\n",
    "            print(e)\n",
    "            return\n",
    "        query_images = query_images.to(device)\n",
    "        query_labels = query_labels.to(device)\n",
    "\n",
    "        query_embeddings = model(query_images)\n",
    "\n",
    "        # Open-set recognition\n",
    "        predicted_labels, min_distances = open_set_recognition(query_embeddings, prototypes, threshold)\n",
    "\n",
    "        # Map unknown classes to -1\n",
    "        true_labels = torch.full_like(query_labels, -1)\n",
    "        accuracy = (predicted_labels == true_labels).float().mean().item()\n",
    "        print(f\"Test Accuracy (Open-Set Recognition): {accuracy * 100:.2f}%\")\n",
    "\n",
    "# 11. Initialize Model and Optimizer\n",
    "feature_extractor = CapsuleNet(output_dim=64).to(device)\n",
    "model = PrototypicalNetwork(feature_extractor).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 12. Training Parameters\n",
    "num_episodes = 1000\n",
    "n_way = 5\n",
    "k_shot = 5\n",
    "k_query = 5\n",
    "\n",
    "# 13. Train the Model\n",
    "train_prototypical_network(model, train_dataset, optimizer, train_class_to_indices,\n",
    "                           num_episodes, n_way, k_shot, k_query)\n",
    "\n",
    "# 14. Test the Model\n",
    "# Adjust testing parameters\n",
    "n_way_test = 5       # Number of classes per episode\n",
    "k_shot_test = 0      # Number of support samples per class in the test set\n",
    "k_query_test = 5     # Number of query samples per class\n",
    "\n",
    "threshold = 1.0  # Adjust based on validation\n",
    "test_prototypical_network(model, train_dataset, train_class_to_indices,\n",
    "                          test_dataset, test_class_to_indices, n_way=n_way_test,\n",
    "                          k_shot=k_shot_test, k_query=k_query_test, threshold=threshold)\n",
    "\n",
    "# 15. Compute Prototypes for Inference\n",
    "with torch.no_grad():\n",
    "    # Use the same n_way and k_shot as during training to compute prototypes\n",
    "    support_images, support_labels, _, _ = create_episode(\n",
    "        train_dataset, train_class_to_indices, n_way=n_way, k_shot=k_shot, k_query=0)\n",
    "    support_images = support_images.to(device)\n",
    "    support_labels = support_labels.to(device)\n",
    "    support_embeddings = model(support_images)\n",
    "    prototypes = compute_prototypes(support_embeddings, support_labels, n_way=n_way, k_shot=k_shot)\n",
    "\n",
    "# 16. Inference with Explainability\n",
    "class GuidedBackpropReLUModel(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super(GuidedBackpropReLUModel, self).__init__()\n",
    "        self.model = model\n",
    "        self.model.eval()\n",
    "        self.update_relus()\n",
    "\n",
    "    def update_relus(self):\n",
    "        def recursive_relu_apply(module_top):\n",
    "            for idx, module in module_top._modules.items():\n",
    "                if isinstance(module, nn.ReLU):\n",
    "                    module_top._modules[idx] = GuidedReLU()\n",
    "                elif len(module._modules) > 0:\n",
    "                    recursive_relu_apply(module)\n",
    "\n",
    "        recursive_relu_apply(self.model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def generate_gradients(self, input_image, target_class):\n",
    "        input_image = input_image.to(device)\n",
    "        input_image.requires_grad = True\n",
    "        output = self.forward(input_image)\n",
    "        self.model.zero_grad()\n",
    "        grad_target_map = torch.zeros_like(output)\n",
    "        grad_target_map[0][target_class] = 1\n",
    "        output.backward(grad_target_map)\n",
    "        gradients_as_arr = input_image.grad.data.cpu().numpy()[0]\n",
    "        return gradients_as_arr\n",
    "\n",
    "class GuidedReLU(nn.Module):\n",
    "    def forward(self, input):\n",
    "        positive_mask = (input > 0).type_as(input)\n",
    "        output = F.relu(input)\n",
    "        return output * positive_mask\n",
    "\n",
    "def inference_with_explainability(model, image, prototypes, threshold, explainer):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        embedding = model(image.to(device))\n",
    "    predicted_labels, distances = open_set_recognition(embedding, prototypes, threshold)\n",
    "    if predicted_labels.item() == -1:\n",
    "        result = \"Unknown\"\n",
    "        target_class = distances.argmin().item()\n",
    "    else:\n",
    "        result = f\"Class {predicted_labels.item()}\"\n",
    "        target_class = predicted_labels.item()\n",
    "    # Generate Saliency Map\n",
    "    saliency_map = explainer.generate_gradients(image, target_class)\n",
    "    saliency_map = np.transpose(saliency_map, (1, 2, 0))\n",
    "    saliency_map = np.mean(np.abs(saliency_map), axis=2)\n",
    "    return result, saliency_map\n",
    "\n",
    "# Initialize the explainer\n",
    "explainer = GuidedBackpropReLUModel(model.feature_extractor)\n",
    "\n",
    "# Select a test image\n",
    "test_image, _ = test_dataset[0]\n",
    "test_image = test_image.unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "# Run inference\n",
    "result, saliency_map = inference_with_explainability(model, test_image, prototypes,\n",
    "                                                     threshold=threshold, explainer=explainer)\n",
    "print(\"Inference Result:\", result)\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(test_image.squeeze().detach().cpu().numpy(), cmap='gray')\n",
    "plt.imshow(saliency_map, cmap='hot', alpha=0.5)\n",
    "plt.title(\"Saliency Map\")\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4515c07e-8280-49b8-89bb-b9b0cfa2800a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data\\cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████| 170498071/170498071 [05:29<00:00, 517300.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\cifar-10-python.tar.gz to ./data\n",
      "Files already downloaded and verified\n",
      "Episode 100/1000, Loss: 1.4457\n",
      "Episode 200/1000, Loss: 1.5218\n",
      "Episode 300/1000, Loss: 1.3720\n",
      "Episode 400/1000, Loss: 1.2105\n",
      "Episode 500/1000, Loss: 1.4201\n",
      "Episode 600/1000, Loss: 1.3894\n",
      "Episode 700/1000, Loss: 1.3661\n",
      "Episode 800/1000, Loss: 1.0593\n",
      "Episode 900/1000, Loss: 1.4096\n",
      "Episode 1000/1000, Loss: 1.1728\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x800 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 5.87%\n",
      "Inference Result: Class 5\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 1. Import Libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms, datasets\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 2. Set Device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# 3. Prepare the CIFAR-10 Dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "train_dataset_full = datasets.CIFAR10(\n",
    "    root='./data',\n",
    "    train=True,\n",
    "    transform=transform,\n",
    "    download=True\n",
    ")\n",
    "\n",
    "test_dataset = datasets.CIFAR10(\n",
    "    root='./data',\n",
    "    train=False,\n",
    "    transform=transform,\n",
    "    download=True\n",
    ")\n",
    "\n",
    "# 4. Limit the Training Dataset to 100 Images per Class\n",
    "def limit_dataset(dataset):\n",
    "    class_to_indices = defaultdict(list)\n",
    "    for idx, (image, label) in enumerate(dataset):\n",
    "        class_to_indices[label].append(idx)\n",
    "\n",
    "    limited_indices = []\n",
    "    for cls in range(10):\n",
    "        indices = class_to_indices[cls][:100]  # Limit to 100 images per class\n",
    "        limited_indices.extend(indices)\n",
    "\n",
    "    limited_dataset = torch.utils.data.Subset(dataset, limited_indices)\n",
    "    return limited_dataset\n",
    "\n",
    "train_dataset = limit_dataset(train_dataset_full)\n",
    "\n",
    "# Rebuild class_to_indices mapping for the limited dataset\n",
    "def extract_labels_and_build_mapping(dataset):\n",
    "    labels = []\n",
    "    for idx in range(len(dataset)):\n",
    "        _, label = dataset[idx]\n",
    "        labels.append(label)\n",
    "    class_to_indices = defaultdict(list)\n",
    "    for idx, label in enumerate(labels):\n",
    "        class_to_indices[label].append(idx)\n",
    "    return labels, class_to_indices\n",
    "\n",
    "train_labels, train_class_to_indices = extract_labels_and_build_mapping(train_dataset)\n",
    "test_labels, test_class_to_indices = extract_labels_and_build_mapping(test_dataset)\n",
    "\n",
    "# 5. Define the Capsule Network (Adjusted for CIFAR-10)\n",
    "class CapsuleNet(nn.Module):\n",
    "    def __init__(self, output_dim):\n",
    "        super(CapsuleNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 256, kernel_size=5, stride=1)\n",
    "        self.primary_caps = nn.Conv2d(256, 8 * 32, kernel_size=5, stride=2)\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.primary_caps(x))\n",
    "        batch_size = x.size(0)\n",
    "        x = x.view(batch_size, -1, 8)\n",
    "        x = self.squash(x)\n",
    "        x = x.view(batch_size, -1)\n",
    "        return x[:, :self.output_dim]\n",
    "\n",
    "    def squash(self, x):\n",
    "        s_squared_norm = (x ** 2).sum(dim=-1, keepdim=True)\n",
    "        scale = s_squared_norm / (1 + s_squared_norm)\n",
    "        return scale * x / torch.sqrt(s_squared_norm + 1e-8)\n",
    "\n",
    "# 6. Define the Prototypical Network\n",
    "class PrototypicalNetwork(nn.Module):\n",
    "    def __init__(self, feature_extractor):\n",
    "        super(PrototypicalNetwork, self).__init__()\n",
    "        self.feature_extractor = feature_extractor\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeddings = self.feature_extractor(x)\n",
    "        return embeddings\n",
    "\n",
    "# 7. Helper Functions\n",
    "def compute_prototypes(embeddings, labels, n_way, k_shot):\n",
    "    prototypes = []\n",
    "    for i in range(n_way):\n",
    "        class_embeddings = embeddings[labels == i]\n",
    "        class_prototype = class_embeddings.mean(0)\n",
    "        prototypes.append(class_prototype)\n",
    "    prototypes = torch.stack(prototypes)\n",
    "    return prototypes\n",
    "\n",
    "def prototypical_loss(query_embeddings, query_labels, prototypes):\n",
    "    distances = euclidean_distances(query_embeddings, prototypes)\n",
    "    log_p_y = F.log_softmax(-distances, dim=1)\n",
    "    loss = -log_p_y.gather(1, query_labels.unsqueeze(1)).mean()\n",
    "    return loss\n",
    "\n",
    "def euclidean_distances(a, b):\n",
    "    n = a.size(0)\n",
    "    m = b.size(0)\n",
    "    a = a.unsqueeze(1).expand(n, m, -1)\n",
    "    b = b.unsqueeze(0).expand(n, m, -1)\n",
    "    distances = ((a - b) ** 2).sum(2)\n",
    "    return distances\n",
    "\n",
    "def open_set_recognition(embeddings, prototypes, threshold):\n",
    "    distances = euclidean_distances(embeddings, prototypes)\n",
    "    min_distances, predicted_labels = distances.min(1)\n",
    "    is_known = min_distances <= threshold\n",
    "    predicted_labels[~is_known] = -1  # Assign -1 to unknown classes\n",
    "    return predicted_labels, min_distances\n",
    "\n",
    "# 8. Create Episode Function\n",
    "def create_episode(dataset, class_to_indices, n_way, k_shot, k_query):\n",
    "    # Filter out classes that don't have enough samples\n",
    "    classes_with_enough_samples = [\n",
    "        cls for cls, idxs in class_to_indices.items() if len(idxs) >= k_shot + k_query\n",
    "    ]\n",
    "    if len(classes_with_enough_samples) < n_way:\n",
    "        raise ValueError(f\"Not enough classes with at least {k_shot + k_query} samples. \"\n",
    "                         f\"Available: {len(classes_with_enough_samples)}, Required: {n_way}\")\n",
    "    selected_classes = random.sample(classes_with_enough_samples, n_way)\n",
    "    support_images = []\n",
    "    query_images = []\n",
    "    support_labels = []\n",
    "    query_labels = []\n",
    "    class_to_idx = {cls: idx for idx, cls in enumerate(selected_classes)}\n",
    "    for cls in selected_classes:\n",
    "        indices = class_to_indices[cls]\n",
    "        selected_indices = random.sample(indices, k_shot + k_query)\n",
    "        support_indices = selected_indices[:k_shot] if k_shot > 0 else []\n",
    "        query_indices = selected_indices[k_shot:] if k_query > 0 else []\n",
    "        for idx in support_indices:\n",
    "            image, _ = dataset[idx]\n",
    "            support_images.append(image)\n",
    "            support_labels.append(class_to_idx[cls])\n",
    "        for idx in query_indices:\n",
    "            image, _ = dataset[idx]\n",
    "            query_images.append(image)\n",
    "            query_labels.append(class_to_idx[cls])\n",
    "    # Only raise an error if k_query > 0 and no query images\n",
    "    if k_query > 0 and len(query_images) == 0:\n",
    "        raise ValueError(\"No query images available. Please adjust k_query or check the dataset.\")\n",
    "    # Only raise an error if k_shot > 0 and no support images\n",
    "    if k_shot > 0 and len(support_images) == 0:\n",
    "        raise ValueError(\"No support images available. Please adjust k_shot or check the dataset.\")\n",
    "    if len(support_images) > 0:\n",
    "        support_images = torch.stack(support_images)\n",
    "        support_labels = torch.tensor(support_labels)\n",
    "    else:\n",
    "        support_images = None\n",
    "        support_labels = None\n",
    "    if len(query_images) > 0:\n",
    "        query_images = torch.stack(query_images)\n",
    "        query_labels = torch.tensor(query_labels)\n",
    "    else:\n",
    "        query_images = None\n",
    "        query_labels = None\n",
    "    return support_images, support_labels, query_images, query_labels\n",
    "\n",
    "# 9. Training Function\n",
    "def train_prototypical_network(model, dataset, optimizer, class_to_indices,\n",
    "                               num_episodes=1000, n_way=5, k_shot=5, k_query=5):\n",
    "    model.train()\n",
    "    for episode in range(num_episodes):\n",
    "        try:\n",
    "            support_images, support_labels, query_images, query_labels = create_episode(\n",
    "                dataset, class_to_indices, n_way, k_shot, k_query)\n",
    "        except ValueError as e:\n",
    "            print(e)\n",
    "            continue  # Skip this episode if not enough classes\n",
    "        support_images = support_images.to(device)\n",
    "        query_images = query_images.to(device)\n",
    "        support_labels = support_labels.to(device)\n",
    "        query_labels = query_labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        # Combine support and query images\n",
    "        images = torch.cat([support_images, query_images], dim=0)\n",
    "        embeddings = model(images)\n",
    "        # Split embeddings\n",
    "        support_embeddings = embeddings[:n_way * k_shot]\n",
    "        query_embeddings = embeddings[n_way * k_shot:]\n",
    "        # Compute prototypes\n",
    "        prototypes = compute_prototypes(support_embeddings, support_labels, n_way, k_shot)\n",
    "        # Compute loss\n",
    "        loss = prototypical_loss(query_embeddings, query_labels, prototypes)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if (episode + 1) % 100 == 0:\n",
    "            print(f\"Episode {episode+1}/{num_episodes}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "# 10. Testing Function (with Confusion Matrix)\n",
    "def test_prototypical_network(model, train_dataset, train_class_to_indices, test_dataset, test_class_to_indices,\n",
    "                              n_way=10, k_shot=0, k_query=5, threshold=1.0):\n",
    "    model.eval()\n",
    "    all_predicted_labels = []\n",
    "    all_true_labels = []\n",
    "    with torch.no_grad():\n",
    "        # Create support set from training dataset\n",
    "        support_images, support_labels, _, _ = create_episode(\n",
    "            train_dataset, train_class_to_indices, n_way, k_shot=5, k_query=0)\n",
    "        support_images = support_images.to(device)\n",
    "        support_labels = support_labels.to(device)\n",
    "        support_embeddings = model(support_images)\n",
    "        prototypes = compute_prototypes(support_embeddings, support_labels, n_way, k_shot=5)\n",
    "        # Iterate over test dataset\n",
    "        for cls in range(10):\n",
    "            indices = test_class_to_indices[cls]\n",
    "            for idx in indices:\n",
    "                image, label = test_dataset[idx]\n",
    "                image = image.unsqueeze(0).to(device)\n",
    "                embedding = model(image)\n",
    "                predicted_label, _ = open_set_recognition(embedding, prototypes, threshold)\n",
    "                predicted_label = predicted_label.item()\n",
    "                all_predicted_labels.append(predicted_label)\n",
    "                all_true_labels.append(cls)\n",
    "    # Replace unknown class labels (-1) with 'Unknown' for confusion matrix\n",
    "    all_predicted_labels = np.array(all_predicted_labels)\n",
    "    all_true_labels = np.array(all_true_labels)\n",
    "    all_predicted_labels[all_predicted_labels == -1] = 10  # Assign 10 to 'Unknown'\n",
    "    # Confusion Matrix\n",
    "    class_names = [str(i) for i in range(10)] + ['Unknown']\n",
    "    cm = confusion_matrix(all_true_labels, all_predicted_labels, labels=list(range(11)))\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', xticklabels=class_names, yticklabels=class_names, cmap='Blues')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "    # Calculate accuracy excluding 'Unknown' class\n",
    "    correct = (all_predicted_labels == all_true_labels)\n",
    "    accuracy = np.mean(correct) * 100\n",
    "    print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "# 11. Initialize Model and Optimizer\n",
    "feature_extractor = CapsuleNet(output_dim=64).to(device)\n",
    "model = PrototypicalNetwork(feature_extractor).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 12. Training Parameters\n",
    "num_episodes = 1000\n",
    "n_way = 5\n",
    "k_shot = 5\n",
    "k_query = 5\n",
    "\n",
    "# 13. Train the Model\n",
    "train_prototypical_network(model, train_dataset, optimizer, train_class_to_indices,\n",
    "                           num_episodes, n_way, k_shot, k_query)\n",
    "\n",
    "# 14. Test the Model\n",
    "# Adjust testing parameters\n",
    "n_way_test = 10      # Number of classes per episode\n",
    "k_shot_test = 0      # Number of support samples per class in the test set\n",
    "k_query_test = 1     # Number of query samples per class\n",
    "\n",
    "threshold = 1.0  # Adjust based on validation\n",
    "test_prototypical_network(model, train_dataset, train_class_to_indices,\n",
    "                          test_dataset, test_class_to_indices, n_way=n_way_test,\n",
    "                          k_shot=k_shot_test, k_query=k_query_test, threshold=threshold)\n",
    "\n",
    "# 15. Compute Prototypes for Inference\n",
    "with torch.no_grad():\n",
    "    # Use the same n_way and k_shot as during training to compute prototypes\n",
    "    support_images, support_labels, _, _ = create_episode(\n",
    "        train_dataset, train_class_to_indices, n_way=10, k_shot=5, k_query=0)\n",
    "    support_images = support_images.to(device)\n",
    "    support_labels = support_labels.to(device)\n",
    "    support_embeddings = model(support_images)\n",
    "    prototypes = compute_prototypes(support_embeddings, support_labels, n_way=10, k_shot=5)\n",
    "\n",
    "# 16. Inference with Explainability\n",
    "class GuidedBackpropReLUModel(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super(GuidedBackpropReLUModel, self).__init__()\n",
    "        self.model = model\n",
    "        self.model.eval()\n",
    "        self.update_relus()\n",
    "\n",
    "    def update_relus(self):\n",
    "        def recursive_relu_apply(module_top):\n",
    "            for idx, module in module_top._modules.items():\n",
    "                if isinstance(module, nn.ReLU):\n",
    "                    module_top._modules[idx] = GuidedReLU()\n",
    "                elif len(module._modules) > 0:\n",
    "                    recursive_relu_apply(module)\n",
    "\n",
    "        recursive_relu_apply(self.model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def generate_gradients(self, input_image, target_class):\n",
    "        input_image = input_image.to(device)\n",
    "        input_image.requires_grad = True\n",
    "        output = self.forward(input_image)\n",
    "        self.model.zero_grad()\n",
    "        grad_target_map = torch.zeros_like(output)\n",
    "        grad_target_map[0][target_class] = 1\n",
    "        output.backward(grad_target_map)\n",
    "        gradients_as_arr = input_image.grad.data.cpu().numpy()[0]\n",
    "        return gradients_as_arr\n",
    "\n",
    "class GuidedReLU(nn.Module):\n",
    "    def forward(self, input):\n",
    "        positive_mask = (input > 0).type_as(input)\n",
    "        output = F.relu(input)\n",
    "        return output * positive_mask\n",
    "\n",
    "def inference_with_explainability(model, image, prototypes, threshold, explainer):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        embedding = model(image.to(device))\n",
    "    predicted_labels, distances = open_set_recognition(embedding, prototypes, threshold)\n",
    "    if predicted_labels.item() == -1:\n",
    "        result = \"Unknown\"\n",
    "        target_class = distances.argmin().item()\n",
    "    else:\n",
    "        result = f\"Class {predicted_labels.item()}\"\n",
    "        target_class = predicted_labels.item()\n",
    "    # Generate Saliency Map\n",
    "    saliency_map = explainer.generate_gradients(image, target_class)\n",
    "    saliency_map = np.transpose(saliency_map, (1, 2, 0))\n",
    "    saliency_map = np.mean(np.abs(saliency_map), axis=2)\n",
    "    return result, saliency_map\n",
    "\n",
    "# Initialize the explainer\n",
    "explainer = GuidedBackpropReLUModel(model.feature_extractor)\n",
    "\n",
    "# Select a test image\n",
    "test_image, _ = test_dataset[0]\n",
    "test_image = test_image.unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "# Run inference\n",
    "result, saliency_map = inference_with_explainability(model, test_image, prototypes,\n",
    "                                                     threshold=threshold, explainer=explainer)\n",
    "print(\"Inference Result:\", result)\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(np.transpose(test_image.squeeze().detach().cpu().numpy(), (1, 2, 0)))\n",
    "plt.imshow(saliency_map, cmap='hot', alpha=0.5)\n",
    "plt.title(\"Saliency Map\")\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b4ddfb-eea7-41be-8135-ff92645b3185",
   "metadata": {},
   "source": [
    "# New"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8a2ccac8-da23-4aaa-8153-7725ed3ff235",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (73728) must match the size of tensor b (1152) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[55], line 198\u001b[0m\n\u001b[0;32m    195\u001b[0m labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m    197\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m--> 198\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    200\u001b[0m \u001b[38;5;66;03m# Compute prototypes\u001b[39;00m\n\u001b[0;32m    201\u001b[0m prototypes \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[55], line 155\u001b[0m, in \u001b[0;36mCapsuleAttentionModel.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    152\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(batch_size, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprimary_capsules_dim)\n\u001b[0;32m    154\u001b[0m \u001b[38;5;66;03m# Digit Capsules\u001b[39;00m\n\u001b[1;32m--> 155\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdigit_capsules\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;66;03m# Reshape for attention\u001b[39;00m\n\u001b[0;32m    158\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(batch_size, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m6\u001b[39m, \u001b[38;5;241m6\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[55], line 75\u001b[0m, in \u001b[0;36mCapsuleLayer.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     73\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     74\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m4\u001b[39m)\n\u001b[1;32m---> 75\u001b[0m priors \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroute_weights\u001b[49m\n\u001b[0;32m     76\u001b[0m logits \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;241m*\u001b[39mpriors\u001b[38;5;241m.\u001b[39msize())\u001b[38;5;241m.\u001b[39mto(x\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m     78\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m3\u001b[39m):\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (73728) must match the size of tensor b (1152) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageDraw\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Custom Dataset\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, known=True, transform=None, num_samples=10):\n",
    "        self.transform = transform\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "        self.known = known\n",
    "        self.num_samples = num_samples\n",
    "        self._create_dataset()\n",
    "        \n",
    "    def _create_dataset(self):\n",
    "        for i in range(self.num_samples):\n",
    "            if self.known:\n",
    "                img = Image.new('RGB', (224, 224), color='white')\n",
    "                draw = ImageDraw.Draw(img)\n",
    "                draw.ellipse((60, 60, 160, 160), fill='gray', outline='black')\n",
    "                self.images.append(img)\n",
    "                self.labels.append(0)  # Class 'Cat'\n",
    "            else:\n",
    "                img = Image.new('RGB', (224, 224), color='white')\n",
    "                draw = ImageDraw.Draw(img)\n",
    "                draw.rectangle((80, 80, 140, 140), fill='orange', outline='black')\n",
    "                self.images.append(img)\n",
    "                self.labels.append(1)  # Class 'Unknown'\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        label = self.labels[idx] if self.known else -1  # -1 for unknown\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "# Data Transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Data Loaders\n",
    "train_dataset = CustomImageDataset(known=True, transform=transform, num_samples=10)\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "unknown_dataset = CustomImageDataset(known=False, transform=transform, num_samples=5)\n",
    "unknown_loader = DataLoader(unknown_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "# Capsule Layer\n",
    "class CapsuleLayer(nn.Module):\n",
    "    def __init__(self, num_capsules, in_channels, out_channels, num_routes):\n",
    "        super(CapsuleLayer, self).__init__()\n",
    "        self.num_routes = num_routes\n",
    "        self.num_capsules = num_capsules\n",
    "\n",
    "        self.route_weights = nn.Parameter(torch.randn(num_capsules, num_routes, in_channels, out_channels))\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        x = x.unsqueeze(1).unsqueeze(4)\n",
    "        priors = x @ self.route_weights\n",
    "        logits = torch.zeros(*priors.size()).to(x.device)\n",
    "\n",
    "        for i in range(3):\n",
    "            probs = F.softmax(logits, dim=2)\n",
    "            outputs = self.squash((probs * priors).sum(dim=2, keepdim=True))\n",
    "            if i != 2:\n",
    "                logits = logits + (priors * outputs).sum(dim=-1, keepdim=True)\n",
    "        return outputs.squeeze(3).squeeze(1)\n",
    "\n",
    "    @staticmethod\n",
    "    def squash(s):\n",
    "        s_norm = (s ** 2).sum(dim=-1, keepdim=True)\n",
    "        scale = s_norm / (1 + s_norm)\n",
    "        return scale * s / torch.sqrt(s_norm + 1e-8)\n",
    "\n",
    "# Attention Mechanism\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, in_dim):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.query_conv = nn.Conv2d(in_dim, in_dim // 8, kernel_size=1)\n",
    "        self.key_conv   = nn.Conv2d(in_dim, in_dim // 8, kernel_size=1)\n",
    "        self.value_conv = nn.Conv2d(in_dim, in_dim, kernel_size=1)\n",
    "        self.softmax    = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, C, width, height = x.size()\n",
    "        proj_query  = self.query_conv(x).view(batch_size, -1, width * height).permute(0, 2, 1)\n",
    "        proj_key    = self.key_conv(x).view(batch_size, -1, width * height)\n",
    "        energy      = torch.bmm(proj_query, proj_key)\n",
    "        attention   = self.softmax(energy)\n",
    "        proj_value  = self.value_conv(x).view(batch_size, -1, width * height)\n",
    "\n",
    "        out = torch.bmm(proj_value, attention.permute(0, 2, 1))\n",
    "        out = out.view(batch_size, C, width, height)\n",
    "        out = out + x\n",
    "        return out\n",
    "\n",
    "# Complete Model\n",
    "class CapsuleAttentionModel(nn.Module):\n",
    "    def __init__(self, num_classes, embedding_dim=256):\n",
    "        super(CapsuleAttentionModel, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        # Base CNN\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=9, stride=1)\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=9, stride=2)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        # Primary Capsules\n",
    "        self.primary_capsules = nn.Conv2d(128, 8 * 32, kernel_size=9, stride=2)\n",
    "        self.num_primary_capsules = 32\n",
    "        self.primary_capsules_dim = 8\n",
    "\n",
    "        # Digit Capsules\n",
    "        self.digit_capsules = CapsuleLayer(num_capsules=num_classes,\n",
    "                                           num_routes=self.num_primary_capsules * 6 * 6,\n",
    "                                           in_channels=self.primary_capsules_dim,\n",
    "                                           out_channels=16)\n",
    "\n",
    "        # Attention Mechanism\n",
    "        self.attention = SelfAttention(in_dim=16 * num_classes)\n",
    "\n",
    "        # Embedding Layer\n",
    "        self.fc = nn.Linear(16 * num_classes * 6 * 6, embedding_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Base CNN\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.relu(self.conv2(x))\n",
    "\n",
    "        # Primary Capsules\n",
    "        x = self.primary_capsules(x)\n",
    "        batch_size = x.size(0)\n",
    "        x = x.view(batch_size, self.num_primary_capsules, self.primary_capsules_dim, x.size(2), x.size(3))\n",
    "        x = x.permute(0, 1, 3, 4, 2).contiguous()\n",
    "        x = x.view(batch_size, -1, self.primary_capsules_dim)\n",
    "\n",
    "        # Digit Capsules\n",
    "        x = self.digit_capsules(x)\n",
    "\n",
    "        # Reshape for attention\n",
    "        x = x.view(batch_size, -1, 6, 6)\n",
    "\n",
    "        # Attention Mechanism\n",
    "        x = self.attention(x)\n",
    "\n",
    "        # Flatten and Embedding\n",
    "        x = x.view(batch_size, -1)\n",
    "        x = self.fc(x)\n",
    "        x = F.normalize(x, dim=1)  # L2 Normalization\n",
    "\n",
    "        return x\n",
    "\n",
    "# Prototypical Loss\n",
    "def prototypical_loss(input, target, prototypes):\n",
    "    distances = torch.cdist(input, prototypes)\n",
    "    target = target.long()\n",
    "    loss = F.cross_entropy(-distances, target)\n",
    "    return loss, distances\n",
    "\n",
    "# Initialize Model and Optimizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "num_classes = 1\n",
    "embedding_dim = 256\n",
    "\n",
    "model = CapsuleAttentionModel(num_classes=num_classes, embedding_dim=embedding_dim).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training Loop\n",
    "num_epochs = 50\n",
    "prototypes = None\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        embeddings = model(images)\n",
    "\n",
    "        # Compute prototypes\n",
    "        prototypes = []\n",
    "        for cls in range(num_classes):\n",
    "            class_embeddings = embeddings[labels == cls]\n",
    "            if len(class_embeddings) > 0:\n",
    "                prototype = class_embeddings.mean(dim=0)\n",
    "                prototypes.append(prototype)\n",
    "            else:\n",
    "                prototypes.append(torch.zeros(embedding_dim).to(device))\n",
    "        prototypes = torch.stack(prototypes)\n",
    "\n",
    "        loss, distances = prototypical_loss(embeddings, labels, prototypes)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# Compute Final Prototypes\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    all_embeddings = []\n",
    "    all_labels = []\n",
    "    for images, labels in train_loader:\n",
    "        images = images.to(device)\n",
    "        embeddings = model(images)\n",
    "        all_embeddings.append(embeddings.cpu())\n",
    "        all_labels.append(labels)\n",
    "\n",
    "    all_embeddings = torch.cat(all_embeddings)\n",
    "    all_labels = torch.cat(all_labels)\n",
    "\n",
    "    prototypes = []\n",
    "    for cls in range(num_classes):\n",
    "        class_embeddings = all_embeddings[all_labels == cls]\n",
    "        prototype = class_embeddings.mean(dim=0)\n",
    "        prototypes.append(prototype)\n",
    "    prototypes = torch.stack(prototypes).to(device)\n",
    "\n",
    "# Set Threshold T\n",
    "with torch.no_grad():\n",
    "    distances = torch.cdist(all_embeddings.to(device), prototypes)\n",
    "    min_distances, _ = distances.min(dim=1)\n",
    "    T = min_distances.max().item() + 0.1\n",
    "    print(f\"Threshold T for open-set recognition: {T:.4f}\")\n",
    "\n",
    "# Inference Function\n",
    "def infer(model, image, prototypes, T):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        embedding = model(image.unsqueeze(0).to(device))\n",
    "        distances = torch.cdist(embedding, prototypes)\n",
    "        min_distance, predicted_class = distances.min(dim=1)\n",
    "        if min_distance.item() <= T:\n",
    "            label = 'Cat'\n",
    "        else:\n",
    "            label = 'Unknown'\n",
    "        return label, min_distance.item(), distances.squeeze().cpu().numpy()\n",
    "\n",
    "# Generate Visual Explanation\n",
    "def generate_visual_explanation(model, image):\n",
    "    model.eval()\n",
    "    attention_map = None\n",
    "    def hook_function(module, input, output):\n",
    "        nonlocal attention_map\n",
    "        attention_map = output\n",
    "    handle = model.attention.register_forward_hook(hook_function)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        _ = model(image.unsqueeze(0).to(device))\n",
    "\n",
    "    handle.remove()\n",
    "    attention_map = attention_map.squeeze(0).cpu()\n",
    "    attention_map = attention_map.mean(dim=0).numpy()\n",
    "    attention_map -= attention_map.min()\n",
    "    attention_map /= attention_map.max()\n",
    "    attention_map = np.uint8(255 * attention_map)\n",
    "    attention_map = Image.fromarray(attention_map).resize((224, 224), Image.BILINEAR)\n",
    "    attention_map = np.array(attention_map)\n",
    "    return attention_map\n",
    "\n",
    "# Test on Known Images\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "for images, labels in train_loader:\n",
    "    images = images.to(device)\n",
    "    for i in range(images.size(0)):\n",
    "        image = images[i]\n",
    "        label, distance, _ = infer(model, image, prototypes, T)\n",
    "        if label == 'Cat':\n",
    "            correct += 1\n",
    "        total += 1\n",
    "\n",
    "print(f\"Accuracy on known classes: {100 * correct / total:.2f}%\")\n",
    "\n",
    "# Test on Unknown Images\n",
    "unknown_correct = 0\n",
    "unknown_total = 0\n",
    "\n",
    "for images, _ in unknown_loader:\n",
    "    images = images.to(device)\n",
    "    for i in range(images.size(0)):\n",
    "        image = images[i]\n",
    "        label, distance, _ = infer(model, image, prototypes, T)\n",
    "        if label == 'Unknown':\n",
    "            unknown_correct += 1\n",
    "        unknown_total += 1\n",
    "\n",
    "print(f\"Accuracy on unknown classes: {100 * unknown_correct / unknown_total:.2f}%\")\n",
    "\n",
    "# Visual Explanation Example\n",
    "image, _ = unknown_dataset[0]\n",
    "transformed_image = transform(image)\n",
    "attention_map = generate_visual_explanation(model, transformed_image)\n",
    "\n",
    "# Display the image and attention map\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "ax[0].imshow(image)\n",
    "ax[0].set_title(\"Input Image\")\n",
    "ax[0].axis('off')\n",
    "\n",
    "ax[1].imshow(image)\n",
    "ax[1].imshow(attention_map, cmap='jet', alpha=0.5)\n",
    "ax[1].set_title(\"Attention Map Overlay\")\n",
    "ax[1].axis('off')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5069a17d-427e-4ffd-8755-f7ee26adbd0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[2, 128, 8]' is invalid for input of size 8192",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[56], line 185\u001b[0m\n\u001b[0;32m    182\u001b[0m labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m    184\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m--> 185\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    187\u001b[0m \u001b[38;5;66;03m# Compute prototype\u001b[39;00m\n\u001b[0;32m    188\u001b[0m prototype \u001b[38;5;241m=\u001b[39m embeddings\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[56], line 146\u001b[0m, in \u001b[0;36mCapsuleAttentionModel.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    143\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(batch_size, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprimary_capsules_dim)\n\u001b[0;32m    145\u001b[0m \u001b[38;5;66;03m# Digit Capsules\u001b[39;00m\n\u001b[1;32m--> 146\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdigit_capsules\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    148\u001b[0m \u001b[38;5;66;03m# Reshape for attention\u001b[39;00m\n\u001b[0;32m    149\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(batch_size, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[56], line 60\u001b[0m, in \u001b[0;36mCapsuleLayer.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     59\u001b[0m     batch_size \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m---> 60\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_routes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_channels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m4\u001b[39m)\n\u001b[0;32m     62\u001b[0m     priors \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroute_weights)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: shape '[2, 128, 8]' is invalid for input of size 8192"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Data preparation\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.4914, 0.4822, 0.4465],\n",
    "                         std=[0.2023, 0.1994, 0.2010])\n",
    "])\n",
    "\n",
    "# Load CIFAR-10 dataset\n",
    "cifar10_train = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                             download=True, transform=transform)\n",
    "cifar10_test = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                            download=True, transform=transform)\n",
    "\n",
    "# Prepare training data (only 'cat' class with 10 samples)\n",
    "cat_class_index = 3  # 'cat' class index in CIFAR-10\n",
    "cat_indices = [i for i, (_, label) in enumerate(cifar10_train) if label == cat_class_index]\n",
    "train_indices = cat_indices[:10]\n",
    "train_dataset = Subset(cifar10_train, train_indices)\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "# Prepare test data with 'cat' images\n",
    "test_cat_indices = [i for i, (_, label) in enumerate(cifar10_test) if label == cat_class_index]\n",
    "test_cat_indices = test_cat_indices[:50]\n",
    "test_dataset = Subset(cifar10_test, test_cat_indices)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "# Prepare unknown data (classes other than 'cat')\n",
    "unknown_indices = [i for i, (_, label) in enumerate(cifar10_test) if label != cat_class_index]\n",
    "unknown_indices = unknown_indices[:50]\n",
    "unknown_dataset = Subset(cifar10_test, unknown_indices)\n",
    "unknown_loader = DataLoader(unknown_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "# Define Capsule Layer\n",
    "class CapsuleLayer(nn.Module):\n",
    "    def __init__(self, num_capsules, in_channels, out_channels, num_routes):\n",
    "        super(CapsuleLayer, self).__init__()\n",
    "        self.num_capsules = num_capsules\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.num_routes = num_routes\n",
    "\n",
    "        self.route_weights = nn.Parameter(torch.randn(num_capsules, num_routes, in_channels, out_channels))\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        x = x.view(batch_size, self.num_routes, self.in_channels)\n",
    "        x = x.unsqueeze(1).unsqueeze(4)\n",
    "        priors = torch.matmul(x, self.route_weights)\n",
    "        logits = torch.zeros_like(priors).to(x.device)\n",
    "\n",
    "        num_iterations = 3\n",
    "        for i in range(num_iterations):\n",
    "            probs = F.softmax(logits, dim=2)\n",
    "            outputs = self.squash((probs * priors).sum(dim=2, keepdim=True))\n",
    "            if i != num_iterations - 1:\n",
    "                delta_logits = (priors * outputs).sum(dim=-1, keepdim=True)\n",
    "                logits = logits + delta_logits\n",
    "        outputs = outputs.squeeze(3).squeeze(1)\n",
    "        return outputs\n",
    "\n",
    "    @staticmethod\n",
    "    def squash(s):\n",
    "        s_norm = torch.sum(s ** 2, dim=-1, keepdim=True)\n",
    "        scale = s_norm / (1 + s_norm)\n",
    "        return scale * s / torch.sqrt(s_norm + 1e-8)\n",
    "\n",
    "# Define Self-Attention Mechanism\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, in_dim):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.in_dim = in_dim\n",
    "        self.query_conv = nn.Conv2d(in_channels=in_dim, out_channels=in_dim // 8, kernel_size=1)\n",
    "        self.key_conv = nn.Conv2d(in_channels=in_dim, out_channels=in_dim // 8, kernel_size=1)\n",
    "        self.value_conv = nn.Conv2d(in_channels=in_dim, out_channels=in_dim, kernel_size=1)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, C, width, height = x.size()\n",
    "        proj_query = self.query_conv(x).view(batch_size, -1, width * height)\n",
    "        proj_key = self.key_conv(x).view(batch_size, -1, width * height)\n",
    "        energy = torch.bmm(proj_query.permute(0, 2, 1), proj_key)\n",
    "        attention = self.softmax(energy)\n",
    "        proj_value = self.value_conv(x).view(batch_size, -1, width * height)\n",
    "\n",
    "        out = torch.bmm(proj_value, attention.permute(0, 2, 1))\n",
    "        out = out.view(batch_size, C, width, height)\n",
    "        out = out + x\n",
    "        return out\n",
    "\n",
    "# Define the Capsule Attention Model\n",
    "class CapsuleAttentionModel(nn.Module):\n",
    "    def __init__(self, num_classes=1, embedding_dim=256):\n",
    "        super(CapsuleAttentionModel, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        # Base CNN\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=5, stride=1)\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=5, stride=2)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        # Primary Capsules\n",
    "        self.primary_capsules = nn.Conv2d(128, 8 * 32, kernel_size=5, stride=2)\n",
    "        self.num_primary_capsules = 32\n",
    "        self.primary_capsules_dim = 8\n",
    "\n",
    "        # Digit Capsules\n",
    "        self.digit_capsules = CapsuleLayer(num_capsules=num_classes,\n",
    "                                           num_routes=self.num_primary_capsules * 2 * 2,\n",
    "                                           in_channels=self.primary_capsules_dim,\n",
    "                                           out_channels=16)\n",
    "\n",
    "        # Attention Mechanism\n",
    "        self.attention = SelfAttention(in_dim=16 * num_classes)\n",
    "\n",
    "        # Embedding Layer\n",
    "        self.fc = nn.Linear(16 * num_classes * 2 * 2, embedding_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Base CNN\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.relu(self.conv2(x))\n",
    "\n",
    "        # Primary Capsules\n",
    "        x = self.primary_capsules(x)\n",
    "        batch_size = x.size(0)\n",
    "        x = x.view(batch_size, self.num_primary_capsules, self.primary_capsules_dim, x.size(2), x.size(3))\n",
    "        x = x.permute(0, 1, 3, 4, 2).contiguous()\n",
    "        x = x.view(batch_size, -1, self.primary_capsules_dim)\n",
    "\n",
    "        # Digit Capsules\n",
    "        x = self.digit_capsules(x)\n",
    "\n",
    "        # Reshape for attention\n",
    "        x = x.view(batch_size, -1, 2, 2)\n",
    "\n",
    "        # Attention Mechanism\n",
    "        x = self.attention(x)\n",
    "\n",
    "        # Flatten and Embedding\n",
    "        x = x.view(batch_size, -1)\n",
    "        x = self.fc(x)\n",
    "        x = F.normalize(x, dim=1)  # L2 Normalization\n",
    "\n",
    "        return x\n",
    "\n",
    "# Define Prototypical Loss\n",
    "def prototypical_loss(inputs, targets, prototypes):\n",
    "    distances = torch.cdist(inputs, prototypes)\n",
    "    targets = targets.long()\n",
    "    loss = F.cross_entropy(-distances, targets)\n",
    "    return loss, distances\n",
    "\n",
    "# Initialize the model and optimizer\n",
    "model = CapsuleAttentionModel(num_classes=1, embedding_dim=256).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 50\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        images = images.to(device)\n",
    "        labels = torch.zeros_like(labels)  # All labels are 0 for 'cat'\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        embeddings = model(images)\n",
    "\n",
    "        # Compute prototype\n",
    "        prototype = embeddings.mean(dim=0, keepdim=True)\n",
    "        prototypes = prototype  # Only one class\n",
    "\n",
    "        loss, distances = prototypical_loss(embeddings, labels, prototypes)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# Compute final prototype\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    all_embeddings = []\n",
    "    all_labels = []\n",
    "    for images, labels in train_loader:\n",
    "        images = images.to(device)\n",
    "        embeddings = model(images)\n",
    "        all_embeddings.append(embeddings.cpu())\n",
    "        all_labels.append(labels)\n",
    "\n",
    "    all_embeddings = torch.cat(all_embeddings)\n",
    "    all_labels = torch.cat(all_labels)\n",
    "\n",
    "    # Since there's only one class, compute prototype\n",
    "    prototype = all_embeddings.mean(dim=0, keepdim=True)\n",
    "    prototypes = prototype.to(device)\n",
    "\n",
    "# Set threshold T\n",
    "with torch.no_grad():\n",
    "    distances = torch.cdist(all_embeddings.to(device), prototypes)\n",
    "    min_distances = distances.squeeze()\n",
    "    T = min_distances.max().item() + 0.1  # Adding a margin\n",
    "    print(f\"Threshold T for open-set recognition: {T:.4f}\")\n",
    "\n",
    "# Define inference function\n",
    "def infer(model, image, prototypes, T):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        embedding = model(image.unsqueeze(0).to(device))\n",
    "        distances = torch.cdist(embedding, prototypes)\n",
    "        min_distance = distances.item()\n",
    "        if min_distance <= T:\n",
    "            label = 'Cat'\n",
    "        else:\n",
    "            label = 'Unknown'\n",
    "        return label, min_distance\n",
    "\n",
    "# Generate visual explanation\n",
    "def generate_visual_explanation(model, image):\n",
    "    model.eval()\n",
    "    attention_map = None\n",
    "\n",
    "    def hook_function(module, input, output):\n",
    "        nonlocal attention_map\n",
    "        attention_map = output\n",
    "\n",
    "    handle = model.attention.register_forward_hook(hook_function)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        _ = model(image.unsqueeze(0).to(device))\n",
    "\n",
    "    handle.remove()\n",
    "    attention_map = attention_map.squeeze(0).cpu()\n",
    "    attention_map = attention_map.mean(dim=0).numpy()\n",
    "    attention_map -= attention_map.min()\n",
    "    attention_map /= attention_map.max()\n",
    "    attention_map = np.uint8(255 * attention_map)\n",
    "    attention_map = Image.fromarray(attention_map).resize((32, 32), Image.BILINEAR)\n",
    "    attention_map = np.array(attention_map)\n",
    "    return attention_map\n",
    "\n",
    "# Test on known images\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "for images, labels in test_loader:\n",
    "    images = images.to(device)\n",
    "    labels = labels.to(device)\n",
    "    label, min_distance = infer(model, images[0], prototypes, T)\n",
    "    if label == 'Cat':\n",
    "        correct += 1\n",
    "    total += 1\n",
    "\n",
    "print(f\"Accuracy on known classes: {100 * correct / total:.2f}%\")\n",
    "\n",
    "# Test on unknown images\n",
    "unknown_correct = 0\n",
    "unknown_total = 0\n",
    "\n",
    "for images, labels in unknown_loader:\n",
    "    images = images.to(device)\n",
    "    label, min_distance = infer(model, images[0], prototypes, T)\n",
    "    if label == 'Unknown':\n",
    "        unknown_correct += 1\n",
    "    unknown_total += 1\n",
    "\n",
    "print(f\"Accuracy on unknown classes: {100 * unknown_correct / unknown_total:.2f}%\")\n",
    "\n",
    "# Visual explanation example\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Select an image from unknown dataset\n",
    "unknown_iter = iter(unknown_loader)\n",
    "images, _ = next(unknown_iter)\n",
    "image = images[0]\n",
    "attention_map = generate_visual_explanation(model, image)\n",
    "\n",
    "# Inverse normalization for visualization\n",
    "inv_transform = transforms.Normalize(\n",
    "    mean=[-0.4914 / 0.2023, -0.4822 / 0.1994, -0.4465 / 0.2010],\n",
    "    std=[1 / 0.2023, 1 / 0.1994, 1 / 0.2010]\n",
    ")\n",
    "\n",
    "image = inv_transform(image)\n",
    "image = image.permute(1, 2, 0).cpu().numpy()\n",
    "image = np.clip(image, 0, 1)\n",
    "\n",
    "# Display the image and attention map\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "ax[0].imshow(image)\n",
    "ax[0].set_title(\"Input Image\")\n",
    "ax[0].axis('off')\n",
    "\n",
    "ax[1].imshow(image)\n",
    "ax[1].imshow(attention_map, cmap='jet', alpha=0.5)\n",
    "ax[1].set_title(\"Attention Map Overlay\")\n",
    "ax[1].axis('off')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "abf5989a-a8fa-485a-8e8b-f455428b7857",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Epoch [1/50], Loss: 63.8804\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Epoch [2/50], Loss: 36.4060\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Epoch [3/50], Loss: 37.6671\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Epoch [4/50], Loss: 26.1014\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Epoch [5/50], Loss: 19.2519\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Epoch [6/50], Loss: 14.4978\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Epoch [7/50], Loss: 22.5973\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Epoch [8/50], Loss: 18.4893\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Epoch [9/50], Loss: 13.9215\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Epoch [10/50], Loss: 9.9601\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Epoch [11/50], Loss: 16.1935\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Epoch [12/50], Loss: 5.3269\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Epoch [13/50], Loss: 8.8919\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Epoch [14/50], Loss: 8.0919\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Epoch [15/50], Loss: 5.9300\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Epoch [16/50], Loss: 3.9451\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Epoch [17/50], Loss: 2.8849\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Epoch [18/50], Loss: 2.8415\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Epoch [19/50], Loss: 2.8425\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Epoch [20/50], Loss: 2.6240\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Epoch [21/50], Loss: 2.6303\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Epoch [22/50], Loss: 2.3305\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Epoch [23/50], Loss: 2.9592\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Epoch [24/50], Loss: 3.1939\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Epoch [25/50], Loss: 3.4039\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Epoch [26/50], Loss: 2.3452\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Epoch [27/50], Loss: 1.8385\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Epoch [28/50], Loss: 1.8112\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Epoch [29/50], Loss: 1.6456\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Epoch [30/50], Loss: 1.7503\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Epoch [31/50], Loss: 1.7181\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Epoch [32/50], Loss: 1.6346\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Epoch [33/50], Loss: 1.7548\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Epoch [34/50], Loss: 1.7675\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Epoch [35/50], Loss: 1.6067\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Epoch [36/50], Loss: 1.6915\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Epoch [37/50], Loss: 1.7737\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Epoch [38/50], Loss: 1.7198\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Epoch [39/50], Loss: 1.7651\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Epoch [40/50], Loss: 1.7509\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Epoch [41/50], Loss: 2.0988\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Epoch [42/50], Loss: 2.0714\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Epoch [43/50], Loss: 2.1006\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Epoch [44/50], Loss: 2.1469\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Epoch [45/50], Loss: 2.1244\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Epoch [46/50], Loss: 2.1419\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Epoch [47/50], Loss: 2.1905\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Epoch [48/50], Loss: 1.7713\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Epoch [49/50], Loss: 2.1652\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Epoch [50/50], Loss: 1.6090\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Shape before digit_capsules: torch.Size([2, 1152, 8])\n",
      "Threshold T for open-set recognition: 5.6030\n",
      "Shape before digit_capsules: torch.Size([1, 1152, 8])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "X1 and X2 must have the same number of columns. X1: 16 X2: 18432",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[58], line 209\u001b[0m\n\u001b[0;32m    207\u001b[0m images \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m    208\u001b[0m labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m--> 209\u001b[0m label, min_distance \u001b[38;5;241m=\u001b[39m \u001b[43minfer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprototypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mT\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m label \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCat\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    211\u001b[0m     correct \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "Cell \u001b[1;32mIn[58], line 194\u001b[0m, in \u001b[0;36minfer\u001b[1;34m(model, image, prototypes, T)\u001b[0m\n\u001b[0;32m    192\u001b[0m embeddings, _ \u001b[38;5;241m=\u001b[39m model(image\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device))\n\u001b[0;32m    193\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m embeddings\u001b[38;5;241m.\u001b[39mview(embeddings\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m--> 194\u001b[0m distances \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcdist\u001b[49m\u001b[43m(\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprototypes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    195\u001b[0m min_distance \u001b[38;5;241m=\u001b[39m distances\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m    196\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m min_distance \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m T:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\functional.py:1330\u001b[0m, in \u001b[0;36mcdist\u001b[1;34m(x1, x2, p, compute_mode)\u001b[0m\n\u001b[0;32m   1327\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m   1328\u001b[0m         cdist, (x1, x2), x1, x2, p\u001b[38;5;241m=\u001b[39mp, compute_mode\u001b[38;5;241m=\u001b[39mcompute_mode)\n\u001b[0;32m   1329\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m compute_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124muse_mm_for_euclid_dist_if_necessary\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m-> 1330\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcdist\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m   1331\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m compute_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124muse_mm_for_euclid_dist\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m   1332\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _VF\u001b[38;5;241m.\u001b[39mcdist(x1, x2, p, \u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: X1 and X2 must have the same number of columns. X1: 16 X2: 18432"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Data preparation\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((28, 28)),  # Resize to match MNIST dimensions\n",
    "    transforms.Grayscale(num_output_channels=1),  # Convert to grayscale\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.1307],\n",
    "                         std=[0.3081])\n",
    "])\n",
    "\n",
    "# Load CIFAR-10 dataset and extract 'cat' class\n",
    "cifar10_train = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                             download=True, transform=transform)\n",
    "cifar10_test = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                            download=True, transform=transform)\n",
    "\n",
    "# Filter out the 'cat' class (class index 3 in CIFAR-10)\n",
    "cat_class_index = 3\n",
    "\n",
    "# Prepare training data (only 'cat' class with 10 samples)\n",
    "cat_indices = [i for i, (_, label) in enumerate(cifar10_train) if label == cat_class_index]\n",
    "train_indices = cat_indices[:10]\n",
    "train_dataset = Subset(cifar10_train, train_indices)\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "# Prepare test data with 'cat' images\n",
    "test_cat_indices = [i for i, (_, label) in enumerate(cifar10_test) if label == cat_class_index]\n",
    "test_cat_indices = test_cat_indices[:50]\n",
    "test_dataset = Subset(cifar10_test, test_cat_indices)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "# Prepare unknown data (classes other than 'cat')\n",
    "unknown_indices = [i for i, (_, label) in enumerate(cifar10_test) if label != cat_class_index]\n",
    "unknown_indices = unknown_indices[:50]\n",
    "unknown_dataset = Subset(cifar10_test, unknown_indices)\n",
    "unknown_loader = DataLoader(unknown_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "# Define Capsule Layer\n",
    "class CapsuleLayer(nn.Module):\n",
    "    def __init__(self, num_capsules, num_route_nodes, in_channels, out_channels):\n",
    "        super(CapsuleLayer, self).__init__()\n",
    "        self.num_capsules = num_capsules\n",
    "\n",
    "        self.route_weights = nn.Parameter(\n",
    "            torch.randn(num_capsules, num_route_nodes, in_channels, out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        x = x.unsqueeze(2)  # Add extra dim for num_capsules\n",
    "        priors = torch.matmul(x.unsqueeze(3), self.route_weights)  # Matrix multiplication\n",
    "\n",
    "        logits = torch.zeros(*priors.size()).to(x.device)\n",
    "        num_iterations = 3\n",
    "        for i in range(num_iterations):\n",
    "            probs = F.softmax(logits, dim=2)\n",
    "            outputs = self.squash((probs * priors).sum(dim=2, keepdim=True))\n",
    "            if i != num_iterations - 1:\n",
    "                delta_logits = (priors * outputs).sum(dim=-1, keepdim=True)\n",
    "                logits = logits + delta_logits\n",
    "        outputs = outputs.squeeze(2).squeeze(3)\n",
    "        return outputs\n",
    "\n",
    "    @staticmethod\n",
    "    def squash(s):\n",
    "        s_norm = (s ** 2).sum(dim=-1, keepdim=True)\n",
    "        scale = s_norm / (1 + s_norm)\n",
    "        return scale * s / torch.sqrt(s_norm + 1e-8)\n",
    "\n",
    "# Define the Capsule Network\n",
    "class CapsuleNet(nn.Module):\n",
    "    def __init__(self, num_classes=1):\n",
    "        super(CapsuleNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 256, kernel_size=9, stride=1)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.primary_capsules = nn.Conv2d(256, 8 * 32, kernel_size=9, stride=2)\n",
    "        self.num_primary_capsules = 32\n",
    "        self.primary_capsules_dim = 8\n",
    "\n",
    "        self.digit_capsules = CapsuleLayer(\n",
    "            num_capsules=num_classes,\n",
    "            num_route_nodes=32 * 6 * 6,\n",
    "            in_channels=8,\n",
    "            out_channels=16\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(16 * num_classes, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(1024, 28 * 28),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "\n",
    "        x = self.primary_capsules(x)\n",
    "        batch_size = x.size(0)\n",
    "        x = x.view(batch_size, self.num_primary_capsules, self.primary_capsules_dim, -1).permute(0, 1, 3, 2)\n",
    "        x = x.contiguous().view(batch_size, self.num_primary_capsules * x.size(2), self.primary_capsules_dim)\n",
    "\n",
    "        print(f\"Shape before digit_capsules: {x.shape}\")  # Debugging shape\n",
    "        x = self.digit_capsules(x)\n",
    "\n",
    "        x = x.squeeze()\n",
    "        x_norm = x.norm(dim=1)\n",
    "        return x, x_norm\n",
    "\n",
    "# Initialize the model and optimizer\n",
    "model = CapsuleNet(num_classes=1).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 50\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    all_embeddings = []\n",
    "    all_labels = []\n",
    "    for images, labels in train_loader:\n",
    "        images = images.to(device)\n",
    "        labels = torch.zeros(images.size(0)).long().to(device)  # All labels are 0 for 'cat'\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        embeddings, _ = model(images)\n",
    "        embeddings = embeddings.view(embeddings.size(0), -1)\n",
    "        all_embeddings.append(embeddings.detach().cpu())\n",
    "        all_labels.append(labels.cpu())\n",
    "\n",
    "        # Compute prototype\n",
    "        prototype = embeddings.mean(dim=0, keepdim=True)\n",
    "        prototypes = prototype  # Only one class\n",
    "\n",
    "        # Compute distances\n",
    "        distances = torch.cdist(embeddings, prototypes)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = F.mse_loss(distances.squeeze(), torch.zeros_like(distances).squeeze())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# Compute final prototype\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    all_embeddings = []\n",
    "    all_labels = []\n",
    "    for images, labels in train_loader:\n",
    "        images = images.to(device)\n",
    "        embeddings, _ = model(images)\n",
    "        embeddings = embeddings.view(embeddings.size(0), -1)\n",
    "        all_embeddings.append(embeddings.cpu())\n",
    "        all_labels.append(labels)\n",
    "\n",
    "    all_embeddings = torch.cat(all_embeddings)\n",
    "    all_labels = torch.cat(all_labels)\n",
    "\n",
    "    # Since there's only one class, compute prototype\n",
    "    prototype = all_embeddings.mean(dim=0, keepdim=True)\n",
    "    prototypes = prototype.to(device)\n",
    "\n",
    "# Set threshold T\n",
    "with torch.no_grad():\n",
    "    distances = torch.cdist(all_embeddings.to(device), prototypes)\n",
    "    min_distances = distances.squeeze()\n",
    "    T = min_distances.max().item() + 0.1  # Adding a margin\n",
    "    print(f\"Threshold T for open-set recognition: {T:.4f}\")\n",
    "\n",
    "# Define inference function\n",
    "def infer(model, image, prototypes, T):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        embeddings, _ = model(image.unsqueeze(0).to(device))\n",
    "        embeddings = embeddings.view(embeddings.size(0), -1)\n",
    "        distances = torch.cdist(embeddings, prototypes)\n",
    "        min_distance = distances.item()\n",
    "        if min_distance <= T:\n",
    "            label = 'Cat'\n",
    "        else:\n",
    "            label = 'Unknown'\n",
    "        return label, min_distance\n",
    "\n",
    "# Test on known images\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "for images, labels in test_loader:\n",
    "    images = images.to(device)\n",
    "    labels = labels.to(device)\n",
    "    label, min_distance = infer(model, images[0], prototypes, T)\n",
    "    if label == 'Cat':\n",
    "        correct += 1\n",
    "    total += 1\n",
    "\n",
    "print(f\"Accuracy on known classes: {100 * correct / total:.2f}%\")\n",
    "\n",
    "# Test on unknown images\n",
    "unknown_correct = 0\n",
    "unknown_total = 0\n",
    "\n",
    "for images, labels in unknown_loader:\n",
    "    images = images.to(device)\n",
    "    label, min_distance = infer(model, images[0], prototypes, T)\n",
    "    if label == 'Unknown':\n",
    "        unknown_correct += 1\n",
    "    unknown_total += 1\n",
    "\n",
    "print(f\"Accuracy on unknown classes: {100 * unknown_correct / unknown_total:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d7c4aa5f-ff77-4030-b602-e8408125da82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Epoch [1/50], Loss: 43.5363\n",
      "Epoch [2/50], Loss: 46.1068\n",
      "Epoch [3/50], Loss: 28.6192\n",
      "Epoch [4/50], Loss: 25.1506\n",
      "Epoch [5/50], Loss: 23.8908\n",
      "Epoch [6/50], Loss: 24.0974\n",
      "Epoch [7/50], Loss: 18.0789\n",
      "Epoch [8/50], Loss: 11.7558\n",
      "Epoch [9/50], Loss: 12.9341\n",
      "Epoch [10/50], Loss: 9.8420\n",
      "Epoch [11/50], Loss: 5.8394\n",
      "Epoch [12/50], Loss: 4.7906\n",
      "Epoch [13/50], Loss: 2.2661\n",
      "Epoch [14/50], Loss: 1.9839\n",
      "Epoch [15/50], Loss: 2.0767\n",
      "Epoch [16/50], Loss: 2.0052\n",
      "Epoch [17/50], Loss: 1.5122\n",
      "Epoch [18/50], Loss: 1.4479\n",
      "Epoch [19/50], Loss: 1.3011\n",
      "Epoch [20/50], Loss: 1.2610\n",
      "Epoch [21/50], Loss: 1.2080\n",
      "Epoch [22/50], Loss: 1.0417\n",
      "Epoch [23/50], Loss: 1.2392\n",
      "Epoch [24/50], Loss: 1.6456\n",
      "Epoch [25/50], Loss: 1.9891\n",
      "Epoch [26/50], Loss: 2.1656\n",
      "Epoch [27/50], Loss: 3.2251\n",
      "Epoch [28/50], Loss: 3.7086\n",
      "Epoch [29/50], Loss: 1.7464\n",
      "Epoch [30/50], Loss: 1.6665\n",
      "Epoch [31/50], Loss: 1.5996\n",
      "Epoch [32/50], Loss: 1.5153\n",
      "Epoch [33/50], Loss: 1.0982\n",
      "Epoch [34/50], Loss: 1.0755\n",
      "Epoch [35/50], Loss: 0.9092\n",
      "Epoch [36/50], Loss: 0.7720\n",
      "Epoch [37/50], Loss: 0.8051\n",
      "Epoch [38/50], Loss: 0.6376\n",
      "Epoch [39/50], Loss: 0.7757\n",
      "Epoch [40/50], Loss: 0.7342\n",
      "Epoch [41/50], Loss: 0.6894\n",
      "Epoch [42/50], Loss: 0.7487\n",
      "Epoch [43/50], Loss: 0.3327\n",
      "Epoch [44/50], Loss: 0.2999\n",
      "Epoch [45/50], Loss: 0.2890\n",
      "Epoch [46/50], Loss: 0.2299\n",
      "Epoch [47/50], Loss: 0.2400\n",
      "Epoch [48/50], Loss: 0.2093\n",
      "Epoch [49/50], Loss: 0.1936\n",
      "Epoch [50/50], Loss: 0.2035\n",
      "Threshold T for open-set recognition: 1.6921\n",
      "Accuracy on known classes: 98.00%\n",
      "Accuracy on unknown classes: 0.00%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "import numpy as np\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Data preparation\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((28, 28)),  # Resize to match MNIST dimensions\n",
    "    transforms.Grayscale(num_output_channels=1),  # Convert to grayscale\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.1307], std=[0.3081])\n",
    "])\n",
    "\n",
    "# Load CIFAR-10 dataset\n",
    "cifar10_train = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                             download=True, transform=transform)\n",
    "cifar10_test = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                            download=True, transform=transform)\n",
    "\n",
    "# Filter 'cat' class (index 3 in CIFAR-10)\n",
    "cat_class_index = 3\n",
    "\n",
    "# Prepare training data\n",
    "cat_indices = [i for i, (_, label) in enumerate(cifar10_train) if label == cat_class_index]\n",
    "train_indices = cat_indices[:10]\n",
    "train_dataset = Subset(cifar10_train, train_indices)\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "# Prepare test data\n",
    "test_cat_indices = [i for i, (_, label) in enumerate(cifar10_test) if label == cat_class_index]\n",
    "test_cat_indices = test_cat_indices[:50]\n",
    "test_dataset = Subset(cifar10_test, test_cat_indices)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "# Prepare unknown data\n",
    "unknown_indices = [i for i, (_, label) in enumerate(cifar10_test) if label != cat_class_index]\n",
    "unknown_indices = unknown_indices[:50]\n",
    "unknown_dataset = Subset(cifar10_test, unknown_indices)\n",
    "unknown_loader = DataLoader(unknown_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "# Define Capsule Layer\n",
    "class CapsuleLayer(nn.Module):\n",
    "    def __init__(self, num_capsules, num_route_nodes, in_channels, out_channels):\n",
    "        super(CapsuleLayer, self).__init__()\n",
    "        self.num_capsules = num_capsules\n",
    "        self.route_weights = nn.Parameter(\n",
    "            torch.randn(num_capsules, num_route_nodes, in_channels, out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        x = x.unsqueeze(2)  # Add extra dim for num_capsules\n",
    "        priors = torch.matmul(x.unsqueeze(3), self.route_weights)\n",
    "\n",
    "        logits = torch.zeros(*priors.size()).to(x.device)\n",
    "        num_iterations = 3\n",
    "        for i in range(num_iterations):\n",
    "            probs = F.softmax(logits, dim=2)\n",
    "            outputs = self.squash((probs * priors).sum(dim=2, keepdim=True))\n",
    "            if i != num_iterations - 1:\n",
    "                delta_logits = (priors * outputs).sum(dim=-1, keepdim=True)\n",
    "                logits = logits + delta_logits\n",
    "        outputs = outputs.squeeze(2).squeeze(3)\n",
    "        return outputs\n",
    "\n",
    "    @staticmethod\n",
    "    def squash(s):\n",
    "        s_norm = (s ** 2).sum(dim=-1, keepdim=True)\n",
    "        scale = s_norm / (1 + s_norm)\n",
    "        return scale * s / torch.sqrt(s_norm + 1e-8)\n",
    "\n",
    "# Define Capsule Network\n",
    "class CapsuleNet(nn.Module):\n",
    "    def __init__(self, num_classes=1):\n",
    "        super(CapsuleNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 256, kernel_size=9, stride=1)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.primary_capsules = nn.Conv2d(256, 8 * 32, kernel_size=9, stride=2)\n",
    "        self.num_primary_capsules = 32\n",
    "        self.primary_capsules_dim = 8\n",
    "\n",
    "        self.digit_capsules = CapsuleLayer(\n",
    "            num_capsules=num_classes,\n",
    "            num_route_nodes=32 * 6 * 6,\n",
    "            in_channels=8,\n",
    "            out_channels=16\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.primary_capsules(x)\n",
    "        batch_size = x.size(0)\n",
    "        x = x.view(batch_size, self.num_primary_capsules, self.primary_capsules_dim, -1).permute(0, 1, 3, 2)\n",
    "        x = x.contiguous().view(batch_size, self.num_primary_capsules * x.size(2), self.primary_capsules_dim)\n",
    "        return self.digit_capsules(x)\n",
    "\n",
    "# Initialize model and optimizer\n",
    "model = CapsuleNet(num_classes=1).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    all_embeddings = []\n",
    "    for images, _ in train_loader:\n",
    "        images = images.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        embeddings = model(images)\n",
    "        embeddings = embeddings.view(embeddings.size(0), -1)\n",
    "        all_embeddings.append(embeddings.detach().cpu())\n",
    "\n",
    "        prototype = embeddings.mean(dim=0, keepdim=True)\n",
    "        distances = torch.cdist(embeddings, prototype)\n",
    "        loss = F.mse_loss(distances.squeeze(), torch.zeros_like(distances).squeeze())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss / len(train_loader):.4f}\")\n",
    "\n",
    "# Calculate prototype and threshold\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    all_embeddings = []\n",
    "    for images, _ in train_loader:\n",
    "        images = images.to(device)\n",
    "        embeddings = model(images)\n",
    "        embeddings = embeddings.view(embeddings.size(0), -1)\n",
    "        all_embeddings.append(embeddings.cpu())\n",
    "    all_embeddings = torch.cat(all_embeddings)\n",
    "    prototype = all_embeddings.mean(dim=0, keepdim=True).to(device)\n",
    "\n",
    "    distances = torch.cdist(all_embeddings.to(device), prototype)\n",
    "    T = distances.max().item() + 0.1\n",
    "    print(f\"Threshold T for open-set recognition: {T:.4f}\")\n",
    "\n",
    "# Define inference function\n",
    "def infer(model, image, prototypes, T):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        embeddings = model(image.unsqueeze(0).to(device))\n",
    "        embeddings = embeddings.view(embeddings.size(0), -1)\n",
    "        prototypes = prototypes.unsqueeze(0)\n",
    "        distances = torch.cdist(embeddings, prototypes)\n",
    "        min_distance = distances.item()\n",
    "        label = 'Cat' if min_distance <= T else 'Unknown'\n",
    "        return label, min_distance\n",
    "\n",
    "# Test on known and unknown data\n",
    "correct_known, total_known = 0, 0\n",
    "for images, _ in test_loader:\n",
    "    label, _ = infer(model, images[0], prototype, T)\n",
    "    if label == 'Cat':\n",
    "        correct_known += 1\n",
    "    total_known += 1\n",
    "\n",
    "correct_unknown, total_unknown = 0, 0\n",
    "for images, _ in unknown_loader:\n",
    "    label, _ = infer(model, images[0], prototype, T)\n",
    "    if label == 'Unknown':\n",
    "        correct_unknown += 1\n",
    "    total_unknown += 1\n",
    "\n",
    "print(f\"Accuracy on known classes: {100 * correct_known / total_known:.2f}%\")\n",
    "print(f\"Accuracy on unknown classes: {100 * correct_unknown / total_unknown:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492acbde-8b2d-418c-97cc-849d1a595e01",
   "metadata": {},
   "source": [
    "# Cifar-10, 500 image per class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ccb553a8-b546-4784-8d82-6be4bd3b7e0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "import numpy as np\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Data preparation\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((28, 28)),\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.1307], std=[0.3081])\n",
    "])\n",
    "\n",
    "# Load CIFAR-10 dataset\n",
    "cifar10_train = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                             download=True, transform=transform)\n",
    "cifar10_test = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                            download=True, transform=transform)\n",
    "\n",
    "# Prepare training and testing datasets\n",
    "train_indices = []\n",
    "test_indices = []\n",
    "\n",
    "for class_idx in range(10):  # Loop over all 10 classes\n",
    "    # Get indices for the current class\n",
    "    class_indices = [i for i, (_, label) in enumerate(cifar10_train) if label == class_idx]\n",
    "    train_indices += class_indices[:500]  # First 50 images for training\n",
    "\n",
    "    test_class_indices = [i for i, (_, label) in enumerate(cifar10_test) if label == class_idx]\n",
    "    test_indices += test_class_indices  # All test images for testing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7b57f018-837e-49aa-b7f0-3c0c89e266cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset datasets for training and testing\n",
    "train_dataset = Subset(cifar10_train, train_indices)\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "test_dataset = Subset(cifar10_test, test_indices)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "42c78b1f-fb09-460a-acaf-4f393b3e668b",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[64], line 81\u001b[0m\n\u001b[0;32m     78\u001b[0m labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     80\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 81\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     82\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m embeddings\u001b[38;5;241m.\u001b[39mview(embeddings\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     84\u001b[0m \u001b[38;5;66;03m# Collect embeddings per class\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[64], line 62\u001b[0m, in \u001b[0;36mCapsuleNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     60\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(batch_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_primary_capsules, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprimary_capsules_dim, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m     61\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mcontiguous()\u001b[38;5;241m.\u001b[39mview(batch_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_primary_capsules \u001b[38;5;241m*\u001b[39m x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m2\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprimary_capsules_dim)\n\u001b[1;32m---> 62\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdigit_capsules\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[64], line 17\u001b[0m, in \u001b[0;36mCapsuleLayer.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     15\u001b[0m num_iterations \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_iterations):\n\u001b[1;32m---> 17\u001b[0m     probs \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msquash((probs \u001b[38;5;241m*\u001b[39m priors)\u001b[38;5;241m.\u001b[39msum(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m num_iterations \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\functional.py:1858\u001b[0m, in \u001b[0;36msoftmax\u001b[1;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[0;32m   1856\u001b[0m     dim \u001b[38;5;241m=\u001b[39m _get_softmax_dim(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msoftmax\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim(), _stacklevel)\n\u001b[0;32m   1857\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1858\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1859\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1860\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msoftmax(dim, dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class CapsuleLayer(nn.Module):\n",
    "    def __init__(self, num_capsules, num_route_nodes, in_channels, out_channels):\n",
    "        super(CapsuleLayer, self).__init__()\n",
    "        self.num_capsules = num_capsules\n",
    "        self.route_weights = nn.Parameter(\n",
    "            torch.randn(num_capsules, num_route_nodes, in_channels, out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        x = x.unsqueeze(2)  # Add extra dim for num_capsules\n",
    "        priors = torch.einsum('bijf,jkfo->biko', x, self.route_weights)\n",
    "\n",
    "        logits = torch.zeros(*priors.size()).to(x.device)\n",
    "        num_iterations = 3\n",
    "        for i in range(num_iterations):\n",
    "            probs = F.softmax(logits, dim=2)\n",
    "            outputs = self.squash((probs * priors).sum(dim=2, keepdim=True))\n",
    "            if i != num_iterations - 1:\n",
    "                delta_logits = (priors * outputs).sum(dim=-1, keepdim=True)\n",
    "                logits = logits + delta_logits\n",
    "\n",
    "        #print(f\"Shape before squeeze: {outputs.shape}\")\n",
    "        outputs = outputs.squeeze(2)  # Remove the third dimension\n",
    "        if outputs.dim() == 4:\n",
    "            outputs = outputs.squeeze(3)  # Remove the fourth dimension if it exists\n",
    "        #print(f\"Shape after squeeze: {outputs.shape}\")\n",
    "        return outputs\n",
    "\n",
    "    @staticmethod\n",
    "    def squash(s):\n",
    "        s_norm = (s ** 2).sum(dim=-1, keepdim=True)\n",
    "        scale = s_norm / (1 + s_norm)\n",
    "        return scale * s / torch.sqrt(s_norm + 1e-8)\n",
    "\n",
    "\n",
    "class CapsuleNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(CapsuleNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 256, kernel_size=9, stride=1)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.primary_capsules = nn.Conv2d(256, 8 * 32, kernel_size=9, stride=2)\n",
    "        self.num_primary_capsules = 32\n",
    "        self.primary_capsules_dim = 8\n",
    "        self.primary_capsules_height = 6  # Based on CIFAR-10 dimensions (28x28)\n",
    "        self.primary_capsules_width = 6\n",
    "\n",
    "        self.digit_capsules = CapsuleLayer(\n",
    "            num_capsules=num_classes,\n",
    "            num_route_nodes=self.num_primary_capsules * self.primary_capsules_height * self.primary_capsules_width,\n",
    "            in_channels=self.primary_capsules_dim,\n",
    "            out_channels=16\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.primary_capsules(x)\n",
    "        batch_size = x.size(0)\n",
    "        x = x.view(batch_size, self.num_primary_capsules, self.primary_capsules_dim, -1).permute(0, 1, 3, 2)\n",
    "        x = x.contiguous().view(batch_size, self.num_primary_capsules * x.size(2), self.primary_capsules_dim)\n",
    "        return self.digit_capsules(x)\n",
    "\n",
    "\n",
    "# Initialize model and optimizer\n",
    "model = CapsuleNet(num_classes=10).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 50\n",
    "class_prototypes = {}\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    class_embeddings = {i: [] for i in range(10)}  # Store embeddings per class\n",
    "    for images, labels in train_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        embeddings = model(images)\n",
    "        embeddings = embeddings.view(embeddings.size(0), -1)\n",
    "\n",
    "        # Collect embeddings per class\n",
    "        for i, label in enumerate(labels):\n",
    "            class_embeddings[label.item()].append(embeddings[i].detach().cpu())\n",
    "\n",
    "        prototype = embeddings.mean(dim=0, keepdim=True)\n",
    "        distances = torch.cdist(embeddings, prototype)\n",
    "        loss = F.mse_loss(distances.squeeze(), torch.zeros_like(distances).squeeze())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # Compute prototypes for each class\n",
    "    with torch.no_grad():\n",
    "        for class_idx, embeddings_list in class_embeddings.items():\n",
    "            class_prototypes[class_idx] = torch.stack(embeddings_list).mean(dim=0).to(device)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss / len(train_loader):.4f}\")\n",
    "\n",
    "\n",
    "# Testing loop with multi-class inference\n",
    "correct, total = 0, 0\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        embeddings = model(images)\n",
    "        embeddings = embeddings.view(embeddings.size(0), -1)\n",
    "\n",
    "        # Calculate distances to each prototype\n",
    "        distances_list = []\n",
    "        for prototype in class_prototypes.values():\n",
    "            # Ensure prototypes and embeddings are compatible\n",
    "            distance = torch.cdist(embeddings, prototype.unsqueeze(0))  # Compute distances\n",
    "            distances_list.append(distance.squeeze(1))  # Squeeze only the relevant dimension\n",
    "\n",
    "        # Stack distances across prototypes\n",
    "        distances = torch.stack(distances_list, dim=1)\n",
    "        #print(f\"Distances shape: {distances.shape}\")  # Debugging shape\n",
    "\n",
    "        # Determine the class with the minimum distance\n",
    "        predicted_labels = distances.argmin(dim=1)\n",
    "        correct += (predicted_labels == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "# Print test accuracy\n",
    "print(f\"Test Accuracy: {100 * correct / total:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ca9bad-5390-4d8f-9cd1-59498d259466",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "\n",
    "# Ensure all prototypes are available\n",
    "prototype_tensor = torch.stack(list(class_prototypes.values())).to(device)  # [num_classes, feature_dim]\n",
    "\n",
    "# Compute pairwise distances between prototypes\n",
    "pairwise_distances = torch.cdist(prototype_tensor, prototype_tensor)  # [num_classes, num_classes]\n",
    "\n",
    "# Convert to CPU for visualization\n",
    "pairwise_distances = pairwise_distances.cpu().detach().numpy()\n",
    "\n",
    "# Visualize the pairwise distance matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(pairwise_distances, annot=True, fmt=\".2f\", cmap=\"coolwarm\", xticklabels=range(10), yticklabels=range(10))\n",
    "plt.title(\"Pairwise Distances Between Class Prototypes\")\n",
    "plt.xlabel(\"Class\")\n",
    "plt.ylabel(\"Class\")\n",
    "plt.show()\n",
    "\n",
    "# Print distances for debugging\n",
    "print(\"Pairwise Prototype Distances:\")\n",
    "print(pairwise_distances)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03239e15-e172-47f5-abd5-8d1b3f19bda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "embeddings = torch.cat([model(images.to(device)).view(images.size(0), -1) for images, _ in test_loader])\n",
    "labels = torch.cat([labels for _, labels in test_loader])\n",
    "\n",
    "tsne = TSNE(n_components=2)\n",
    "reduced_embeddings = tsne.fit_transform(embeddings.cpu().numpy())\n",
    "\n",
    "plt.scatter(reduced_embeddings[:, 0], reduced_embeddings[:, 1], c=labels, cmap=\"tab10\")\n",
    "plt.colorbar()\n",
    "plt.title(\"t-SNE Visualization of Embeddings\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da1f599-3350-4679-b305-d091c2e0f7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "import numpy as np\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Data preparation\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((28, 28)),\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.1307], std=[0.3081])\n",
    "])\n",
    "\n",
    "# Load CIFAR-10 dataset\n",
    "cifar10_train = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                             download=True, transform=transform)\n",
    "cifar10_test = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                            download=True, transform=transform)\n",
    "\n",
    "# Prepare training and testing datasets\n",
    "train_indices = []\n",
    "test_indices = []\n",
    "\n",
    "for class_idx in range(10):  # Loop over all 10 classes\n",
    "    # Get indices for the current class\n",
    "    class_indices = [i for i, (_, label) in enumerate(cifar10_train) if label == class_idx]\n",
    "    train_indices += class_indices[:200]  # First 50 images for training\n",
    "\n",
    "    test_class_indices = [i for i, (_, label) in enumerate(cifar10_test) if label == class_idx]\n",
    "    test_indices += test_class_indices  # All test images for testing\n",
    "\n",
    "\n",
    "\n",
    "# Subset datasets for training and testing\n",
    "train_dataset = Subset(cifar10_train, train_indices)\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "test_dataset = Subset(cifar10_test, test_indices)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "\n",
    "class CapsuleLayer(nn.Module):\n",
    "    def __init__(self, num_capsules, num_route_nodes, in_channels, out_channels):\n",
    "        super(CapsuleLayer, self).__init__()\n",
    "        self.num_capsules = num_capsules\n",
    "        self.route_weights = nn.Parameter(\n",
    "            torch.randn(num_capsules, num_route_nodes, in_channels, out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        x = x.unsqueeze(2)  # Add extra dim for num_capsules\n",
    "        priors = torch.einsum('bijf,jkfo->biko', x, self.route_weights)\n",
    "\n",
    "        logits = torch.zeros(*priors.size()).to(x.device)\n",
    "        num_iterations = 3\n",
    "        for i in range(num_iterations):\n",
    "            probs = F.softmax(logits, dim=2)\n",
    "            outputs = self.squash((probs * priors).sum(dim=2, keepdim=True))\n",
    "            if i != num_iterations - 1:\n",
    "                delta_logits = (priors * outputs).sum(dim=-1, keepdim=True)\n",
    "                logits = logits + delta_logits\n",
    "\n",
    "        #print(f\"Shape before squeeze: {outputs.shape}\")\n",
    "        outputs = outputs.squeeze(2)  # Remove the third dimension\n",
    "        if outputs.dim() == 4:\n",
    "            outputs = outputs.squeeze(3)  # Remove the fourth dimension if it exists\n",
    "        #print(f\"Shape after squeeze: {outputs.shape}\")\n",
    "        return outputs\n",
    "\n",
    "    @staticmethod\n",
    "    def squash(s):\n",
    "        s_norm = (s ** 2).sum(dim=-1, keepdim=True)\n",
    "        scale = s_norm / (1 + s_norm)\n",
    "        return scale * s / torch.sqrt(s_norm + 1e-8)\n",
    "\n",
    "\n",
    "class CapsuleNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(CapsuleNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 256, kernel_size=9, stride=1)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.primary_capsules = nn.Conv2d(256, 8 * 32, kernel_size=9, stride=2)\n",
    "        self.num_primary_capsules = 32\n",
    "        self.primary_capsules_dim = 8\n",
    "        self.primary_capsules_height = 6  # Based on CIFAR-10 dimensions (28x28)\n",
    "        self.primary_capsules_width = 6\n",
    "\n",
    "        self.digit_capsules = CapsuleLayer(\n",
    "            num_capsules=num_classes,\n",
    "            num_route_nodes=self.num_primary_capsules * self.primary_capsules_height * self.primary_capsules_width,\n",
    "            in_channels=self.primary_capsules_dim,\n",
    "            out_channels=16\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.primary_capsules(x)\n",
    "        batch_size = x.size(0)\n",
    "        x = x.view(batch_size, self.num_primary_capsules, self.primary_capsules_dim, -1).permute(0, 1, 3, 2)\n",
    "        x = x.contiguous().view(batch_size, self.num_primary_capsules * x.size(2), self.primary_capsules_dim)\n",
    "        return self.digit_capsules(x)\n",
    "\n",
    "\n",
    "# Initialize model and optimizer\n",
    "model = CapsuleNet(num_classes=10).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 50\n",
    "class_prototypes = {}\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    class_embeddings = {i: [] for i in range(10)}  # Store embeddings per class\n",
    "    for images, labels in train_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        embeddings = model(images)\n",
    "        embeddings = embeddings.view(embeddings.size(0), -1)\n",
    "\n",
    "        # Collect embeddings per class\n",
    "        for i, label in enumerate(labels):\n",
    "            class_embeddings[label.item()].append(embeddings[i].detach().cpu())\n",
    "\n",
    "        prototype = embeddings.mean(dim=0, keepdim=True)\n",
    "        distances = torch.cdist(embeddings, prototype)\n",
    "        loss = F.mse_loss(distances.squeeze(), torch.zeros_like(distances).squeeze())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # Compute prototypes for each class\n",
    "    with torch.no_grad():\n",
    "        for class_idx, embeddings_list in class_embeddings.items():\n",
    "            class_prototypes[class_idx] = torch.stack(embeddings_list).mean(dim=0).to(device)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss / len(train_loader):.4f}\")\n",
    "\n",
    "\n",
    "# Testing loop with multi-class inference\n",
    "correct, total = 0, 0\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        embeddings = model(images)\n",
    "        embeddings = embeddings.view(embeddings.size(0), -1)\n",
    "\n",
    "        # Calculate distances to each prototype\n",
    "        distances_list = []\n",
    "        for prototype in class_prototypes.values():\n",
    "            # Ensure prototypes and embeddings are compatible\n",
    "            distance = torch.cdist(embeddings, prototype.unsqueeze(0))  # Compute distances\n",
    "            distances_list.append(distance.squeeze(1))  # Squeeze only the relevant dimension\n",
    "\n",
    "        # Stack distances across prototypes\n",
    "        distances = torch.stack(distances_list, dim=1)\n",
    "        #print(f\"Distances shape: {distances.shape}\")  # Debugging shape\n",
    "\n",
    "        # Determine the class with the minimum distance\n",
    "        predicted_labels = distances.argmin(dim=1)\n",
    "        correct += (predicted_labels == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "# Print test accuracy\n",
    "print(f\"Test Accuracy: {100 * correct / total:.2f}%\")\n",
    "\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "\n",
    "# Ensure all prototypes are available\n",
    "prototype_tensor = torch.stack(list(class_prototypes.values())).to(device)  # [num_classes, feature_dim]\n",
    "\n",
    "# Compute pairwise distances between prototypes\n",
    "pairwise_distances = torch.cdist(prototype_tensor, prototype_tensor)  # [num_classes, num_classes]\n",
    "\n",
    "# Convert to CPU for visualization\n",
    "pairwise_distances = pairwise_distances.cpu().detach().numpy()\n",
    "\n",
    "# Visualize the pairwise distance matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(pairwise_distances, annot=True, fmt=\".2f\", cmap=\"coolwarm\", xticklabels=range(10), yticklabels=range(10))\n",
    "plt.title(\"Pairwise Distances Between Class Prototypes\")\n",
    "plt.xlabel(\"Class\")\n",
    "plt.ylabel(\"Class\")\n",
    "plt.show()\n",
    "\n",
    "# Print distances for debugging\n",
    "print(\"Pairwise Prototype Distances:\")\n",
    "print(pairwise_distances)\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
